---
phase: 02-table-export-via-pins
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/export_pins.R
autonomous: false

must_haves:
  truths:
    - "All non-spatial tables are available as pins on S3 under the pins/ prefix"
    - "Each pin has parquet format data with correct types"
    - "Each pin has custom metadata containing table description and column descriptions"
    - "The 19M-row EPC table exports without OOM or timeout"
    - "pin_list(board) from R returns all exported table names"
  artefacts:
    - path: "scripts/export_pins.R"
      provides: "Bulk export of all non-spatial tables as pins to S3"
      contains: "pin_write"
  key_links:
    - from: "scripts/export_pins.R"
      to: "data/mca_env_base.duckdb"
      via: "DBI::dbConnect read-only"
      pattern: "dbConnect.*duckdb.*mca_env_base"
    - from: "scripts/export_pins.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "board_s3 with prefix pins/"
      pattern: 'board_s3.*prefix.*pins'
---

<objective>
Export all non-spatial tables from the source DuckDB as pins to S3 with metadata, using a large-table strategy for the EPC table.

Purpose: This is the core deliverable of Phase 2 -- making all non-spatial datasets available as pins on S3. After this plan, analysts can discover and read datasets from R. The large-table (EPC) export uses `pin_upload()` with DuckDB `COPY TO` to avoid R memory issues.

Output: Export script, all non-spatial tables as pins on S3 with metadata.
</objective>

<execution_context>
@C:\Users\steve.crawshaw\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\steve.crawshaw\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-table-export-via-pins/02-RESEARCH.md
@.planning/phases/02-table-export-via-pins/02-01-SUMMARY.md
@aws_setup.r
</context>

<tasks>

<task type="auto">
  <name>Task 1: Bulk pin export script with large-table handling</name>
  <files>scripts/export_pins.R</files>
  <action>
Create an R script `scripts/export_pins.R` that exports all non-spatial tables from the source DuckDB as pins to S3.

**Configuration (top of script):**
```r
SOURCE_DB <- "data/mca_env_base.duckdb"
S3_BUCKET <- "stevecrawshaw-bucket"
S3_PREFIX <- "pins/"
S3_REGION <- "eu-west-2"
LARGE_TABLE_THRESHOLD <- 5000000  # rows; tables above this use pin_upload
```

**Libraries:** `library(pins)`, `library(duckdb)`, `library(DBI)`, `library(arrow)`. Do NOT use pacman.

**Logic:**

1. Connect to source DuckDB (read-only).
2. Create S3 board: `board_s3(bucket, prefix, region, versioned = TRUE)`.
3. Query `duckdb_tables()` for all non-internal main-schema tables with comments and estimated_size.
4. Query `duckdb_columns()` for all column metadata.
5. Identify and exclude spatial tables (columns with data_type matching BLOB/GEOMETRY/WKB).
6. For each non-spatial table, determine export strategy:
   - **Standard tables** (below threshold): Use `dbReadTable()` then `pin_write(board, df, name, type = "parquet", title, description, metadata)`.
   - **Large tables** (above threshold): Use DuckDB `COPY TO` to write a temporary parquet file (in `tempdir()`), then `pin_upload(board, paths, name, title, description, metadata)`. The temp parquet filename MUST end in `.parquet` (pins uses extension for type detection). Use `ROW_GROUP_SIZE 100000` in the COPY statement.

7. Custom metadata for every pin:
```r
metadata = list(
  source_db = "ducklake",
  columns = setNames(
    as.list(ifelse(is.na(col_meta$comment), "", col_meta$comment)),
    col_meta$column_name
  ),
  column_types = setNames(
    as.list(col_meta$data_type),
    col_meta$column_name
  )
)
```

8. After each table, print progress: `"Exported: {table_name} ({nrow} rows) [{method}]"` where method is "pin_write" or "pin_upload".
9. After all tables, print a summary: total tables exported, total rows, any failures.
10. Run `pin_list(board)` and print the full list to confirm all pins are visible.
11. Disconnect from DuckDB with `shutdown = TRUE`.

**Error handling:** Wrap each table export in `tryCatch`. If one table fails, log the error and continue with the next. Print all failures at the end.

**Important:** The test pin from 02-01 may already exist on the board. That is fine -- `pin_write` will create a new version. Do NOT skip tables that already have a pin.

**Important:** Use `on.exit(unlink(temp_path))` after creating temp parquet files for large tables, so they are cleaned up even if the export fails.
  </action>
  <verify>
Run `Rscript scripts/export_pins.R` and confirm:
- All non-spatial tables are exported (check the printed summary)
- Large tables use pin_upload (check the printed method)
- `pin_list(board)` at the end shows all expected table names
- No failures in the summary
  </verify>
  <done>All non-spatial tables are exported as pins to S3 with metadata. The script completes without OOM. pin_list shows all tables.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>All non-spatial tables exported as pins to S3 with metadata. The export script ran and reported success for all tables including the large EPC table.</what-built>
  <how-to-verify>
1. Check the script output -- all tables should show "Exported" with row counts.
2. Optionally run `Rscript -e "library(pins); b <- board_s3('stevecrawshaw-bucket', prefix='pins/', region='eu-west-2'); print(pin_list(b))"` to see all pins.
3. Optionally spot-check one pin: `Rscript -e "library(pins); b <- board_s3('stevecrawshaw-bucket', prefix='pins/', region='eu-west-2'); print(str(pin_meta(b, 'SOME_TABLE')$user))"` to verify metadata.
4. Confirm the EPC table (19M rows) exported without error.
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues</resume-signal>
</task>

</tasks>

<verification>
1. `Rscript scripts/export_pins.R` completes without OOM or timeout
2. Script output shows all non-spatial tables exported with row counts
3. Large tables (EPC) used pin_upload method
4. `pin_list(board)` returns all expected table names
5. Spot-check: at least one pin has correct custom metadata (columns dict)
</verification>

<success_criteria>
- All non-spatial tables from source DuckDB are pins on S3 under pins/ prefix
- Each pin has parquet data, table description as title, column descriptions in user metadata
- The 19M-row EPC table exported successfully using the file-based pin_upload strategy
- No export failures
</success_criteria>

<output>
After completion, create `.planning/phases/02-table-export-via-pins/02-02-SUMMARY.md`
</output>
