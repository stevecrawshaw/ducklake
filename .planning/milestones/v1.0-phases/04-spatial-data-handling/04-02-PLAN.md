---
phase: 04-spatial-data-handling
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - scripts/recreate_spatial_ducklake.sql
  - scripts/export_spatial_pins.R
autonomous: true

must_haves:
  truths:
    - "All 8 spatial tables exist in DuckLake with native GEOMETRY columns (not BLOB)"
    - "ca_boundaries_bgc_tbl geometry column contains only MULTIPOLYGON (promoted via ST_Multi)"
    - "lsoa_2021_lep_tbl has a geom_valid BOOLEAN column flagging 2 invalid geometries"
    - "Spatial SQL (ST_Contains, ST_Intersects, ST_Area) works on all 8 DuckLake tables"
    - "All 8 spatial tables are uploaded as GeoParquet pins with spatial=TRUE metadata"
    - "Each pin's metadata includes geometry_column, geometry_type, and crs"
  artefacts:
    - path: "scripts/recreate_spatial_ducklake.sql"
      provides: "SQL to drop BLOB versions and recreate all 8 spatial tables with native GEOMETRY"
    - path: "scripts/export_spatial_pins.R"
      provides: "R script to export all 8 spatial tables as GeoParquet pins to S3"
  key_links:
    - from: "scripts/recreate_spatial_ducklake.sql"
      to: "data/mca_env.ducklake"
      via: "DROP + CREATE TABLE with ST_GeomFromWKB/ST_Multi for all 8 tables"
    - from: "scripts/export_spatial_pins.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "GeoParquet COPY TO + pin_upload for all 8 tables"
---

<objective>
Recreate all 8 spatial tables in DuckLake with native GEOMETRY columns and export them as GeoParquet pins to S3. Handle per-table edge cases: ST_Multi() for ca_boundaries_bgc_tbl (mixed types), geom_valid flag for lsoa_2021_lep_tbl (2 invalid geometries).

Purpose: Complete the spatial data handling phase -- all spatial tables become queryable with spatial SQL in DuckLake and accessible as GeoParquet through pins.
Output: All 8 DuckLake spatial tables with GEOMETRY type, all 8 GeoParquet pins on S3.
</objective>

<execution_context>
@C:\Users\steve.crawshaw\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\steve.crawshaw\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-spatial-data-handling/04-RESEARCH.md
@.planning/phases/04-spatial-data-handling/04-01-SUMMARY.md
@scripts/create_ducklake.R
@scripts/create_ducklake.sql
@scripts/export_pins.R
@scripts/spike_spatial.R
@scripts/spike_spatial.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Recreate all 8 spatial DuckLake tables with native GEOMETRY</name>
  <files>scripts/recreate_spatial_ducklake.sql</files>
  <action>
Create `scripts/recreate_spatial_ducklake.sql` that recreates all 8 spatial tables in DuckLake with native GEOMETRY columns. The spike (04-01) already handled bdline_ua_lep_diss_tbl, but this script recreates ALL 8 for idempotency.

The SQL must:
1. Install and load extensions: ducklake, httpfs, aws, spatial
2. Create S3 credential (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain)
3. Attach DuckLake: `ATTACH 'ducklake:data/mca_env.ducklake' AS lake (DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/')`
4. Attach source: `ATTACH 'data/mca_env_base.duckdb' AS source (READ_ONLY)`
5. For each of the 8 spatial tables, DROP IF EXISTS then CREATE TABLE with the appropriate conversion:

**Standard WKB_BLOB tables (6 tables)** -- use `ST_GeomFromWKB(shape) AS shape`:
- bdline_ua_lep_diss_tbl (1 row, POLYGON, EPSG:27700)
- bdline_ua_lep_tbl (4 rows, MULTIPOLYGON, EPSG:27700)
- bdline_ua_weca_diss_tbl (1 row, POLYGON, EPSG:27700)
- bdline_ward_lep_tbl (130 rows, MULTIPOLYGON, EPSG:27700)
- codepoint_open_lep_tbl (31,299 rows, POINT, EPSG:27700)
- open_uprn_lep_tbl (687,143 rows, POINT, EPSG:27700)

Pattern:
```sql
DROP TABLE IF EXISTS lake.{table_name};
CREATE TABLE lake.{table_name} AS
  SELECT * EXCLUDE(shape), ST_GeomFromWKB(shape) AS shape
  FROM source.{table_name};
```

**Mixed geometry type table (1 table)** -- use `ST_Multi(geom) AS geom`:
- ca_boundaries_bgc_tbl (15 rows, mixed POLYGON/MULTIPOLYGON, EPSG:4326)

Note: ca_boundaries_bgc_tbl has a `geom` column (not `shape`) and the source type is already GEOMETRY (not WKB_BLOB). So the conversion is `ST_Multi(geom)` not `ST_GeomFromWKB(geom)`.

Pattern:
```sql
DROP TABLE IF EXISTS lake.ca_boundaries_bgc_tbl;
CREATE TABLE lake.ca_boundaries_bgc_tbl AS
  SELECT * EXCLUDE(geom), ST_Multi(geom) AS geom
  FROM source.ca_boundaries_bgc_tbl;
```

**Table with invalid geometries (1 table)** -- add `geom_valid` flag:
- lsoa_2021_lep_tbl (698 rows, MULTIPOLYGON, EPSG:27700, 2 invalid)

Pattern:
```sql
DROP TABLE IF EXISTS lake.lsoa_2021_lep_tbl;
CREATE TABLE lake.lsoa_2021_lep_tbl AS
  SELECT * EXCLUDE(shape),
    ST_GeomFromWKB(shape) AS shape,
    ST_IsValid(ST_GeomFromWKB(shape)) AS geom_valid
  FROM source.lsoa_2021_lep_tbl;
```

6. After all tables are created, run verification queries:
   - `SELECT table_name, column_name, data_type FROM information_schema.columns WHERE table_catalog = 'lake' AND column_name IN ('shape', 'geom', 'geom_valid') ORDER BY table_name, column_name` -- confirm GEOMETRY type
   - `SELECT COUNT(*) AS invalid_count FROM lake.lsoa_2021_lep_tbl WHERE geom_valid = false` -- should return 2
   - `SELECT ST_GeometryType(geom) AS geom_type, COUNT(*) FROM lake.ca_boundaries_bgc_tbl GROUP BY geom_type` -- should show only MULTIPOLYGON
   - `SELECT COUNT(*) AS total_rows FROM lake.open_uprn_lep_tbl` -- should return 687143 (largest table integrity check)

Execute the SQL via DuckDB CLI using the same R wrapper pattern from the spike. The R execution can be inline in the export script (Task 2) or a separate wrapper -- use the simpler approach.

Important: The open_uprn_lep_tbl has 687K rows. This is the largest table and may take longer to create. It fits in memory (~18MB GeoParquet) so no chunking is needed, but allow adequate timeout (600 seconds).
  </action>
  <verify>
After SQL execution, verify via DuckDB CLI:
1. All 8 spatial tables have GEOMETRY type columns (not BLOB)
2. `lsoa_2021_lep_tbl` has `geom_valid` column with exactly 2 false values
3. `ca_boundaries_bgc_tbl` has only MULTIPOLYGON geometry types
4. `open_uprn_lep_tbl` has 687,143 rows
  </verify>
  <done>
All 8 spatial tables in DuckLake have native GEOMETRY columns. Mixed types promoted to MULTIPOLYGON. Invalid geometries flagged with geom_valid column.
  </done>
</task>

<task type="auto">
  <name>Task 2: Export all 8 spatial tables as GeoParquet pins and validate</name>
  <files>scripts/export_spatial_pins.R</files>
  <action>
Create `scripts/export_spatial_pins.R` that:
1. Executes `scripts/recreate_spatial_ducklake.sql` via DuckDB CLI (same pattern as spike)
2. For each of the 8 spatial tables, exports a GeoParquet file and uploads as a pin

The export loop for each table must:
a. Use DuckDB CLI to run `COPY (SELECT * FROM source.{table}) TO '{temp_path}' (FORMAT PARQUET)` with the appropriate geometry conversion per table. Source the data from `source` (mca_env_base.duckdb), not from `lake`, to avoid reading back from S3. Apply the same conversions as Task 1:
   - Standard tables: `SELECT * EXCLUDE(shape), ST_GeomFromWKB(shape) AS shape FROM source.{table}`
   - ca_boundaries_bgc_tbl: `SELECT * EXCLUDE(geom), ST_Multi(geom) AS geom FROM source.{table}`
   - lsoa_2021_lep_tbl: `SELECT * EXCLUDE(shape), ST_GeomFromWKB(shape) AS shape, ST_IsValid(ST_GeomFromWKB(shape)) AS geom_valid FROM source.{table}`

b. Upload the GeoParquet file via `pin_upload()` with metadata:
   ```r
   pin_upload(board, paths = temp_path, name = table_name,
     title = table_title,
     description = paste0(table_title, " (", nrow, " rows, GeoParquet, ", crs, ")"),
     metadata = list(
       source_db = "ducklake",
       spatial = TRUE,
       geometry_column = geom_col,  # "shape" or "geom"
       geometry_type = geom_type,   # "POLYGON", "MULTIPOLYGON", "POINT"
       crs = crs                    # "EPSG:27700" or "EPSG:4326"
     ))
   ```

c. Clean up the temp GeoParquet file after upload.

Table metadata for the loop (define as a data frame or list in R):

| table_name | geom_col | geom_type | crs | rows | title |
|---|---|---|---|---|---|
| bdline_ua_lep_diss_tbl | shape | POLYGON | EPSG:27700 | 1 | Boundary line UA LEP dissolved |
| bdline_ua_lep_tbl | shape | MULTIPOLYGON | EPSG:27700 | 4 | Boundary line UA LEP |
| bdline_ua_weca_diss_tbl | shape | POLYGON | EPSG:27700 | 1 | Boundary line UA WECA dissolved |
| bdline_ward_lep_tbl | shape | MULTIPOLYGON | EPSG:27700 | 130 | Boundary line ward LEP |
| ca_boundaries_bgc_tbl | geom | MULTIPOLYGON | EPSG:4326 | 15 | Combined authority boundaries BGC |
| codepoint_open_lep_tbl | shape | POINT | EPSG:27700 | 31299 | Codepoint open LEP |
| lsoa_2021_lep_tbl | shape | MULTIPOLYGON | EPSG:27700 | 698 | LSOA 2021 LEP |
| open_uprn_lep_tbl | shape | POINT | EPSG:27700 | 687143 | Open UPRN LEP |

3. After all uploads, validate:
   - List all pins and confirm all 8 spatial tables exist
   - For each spatial pin, confirm `pin_meta(board, name)$user$spatial` is TRUE
   - Pick one table (bdline_ua_lep_diss_tbl), download and read with sfarrow or geoarrow (whichever worked in the spike) as a quick R roundtrip check
   - Print a summary table: table name, rows, geometry type, CRS, pin upload status

4. Print final summary with pass/fail for:
   - DuckLake tables recreated (8/8)
   - GeoParquet pins uploaded (8/8)
   - Spatial metadata confirmed (8/8)
   - R roundtrip check passed (1/1)

Key considerations:
- Use `uv run python` for any Python calls (project convention)
- Temp files go in `data/` directory (not system temp -- avoids Windows path issues)
- Each COPY TO runs as a separate DuckDB CLI call to avoid holding connections open
- The DuckDB CLI calls need extensions loaded: spatial, httpfs, aws, and the source attached. Write a helper function that generates the full SQL for a single table export.
- Error handling: if one table fails, log the error and continue to the next. Report failures at the end.
  </action>
  <verify>
Run `Rscript scripts/export_spatial_pins.R` from the project root. Expected output:
1. All 8 DuckLake spatial tables recreated with GEOMETRY type
2. All 8 GeoParquet files exported and uploaded as pins
3. All 8 pins have spatial=TRUE metadata
4. R roundtrip check passes for bdline_ua_lep_diss_tbl
5. Summary shows 8/8 success for all checks
  </verify>
  <done>
- All 8 spatial tables in DuckLake have native GEOMETRY columns
- All 8 spatial tables are available as GeoParquet pins on S3
- Each pin has spatial=TRUE, geometry_column, geometry_type, and crs in metadata
- ca_boundaries_bgc_tbl has uniform MULTIPOLYGON type
- lsoa_2021_lep_tbl has geom_valid column
- R roundtrip validated for at least one table
  </done>
</task>

</tasks>

<verification>
After running `Rscript scripts/export_spatial_pins.R`:
1. DuckLake verification: All 8 spatial tables have GEOMETRY column type (not BLOB)
2. Pin verification: All 8 spatial pins exist on S3 with spatial=TRUE metadata
3. Spatial SQL verification: `SELECT ST_Area(shape) FROM lake.bdline_ua_lep_diss_tbl` returns non-zero value
4. R consumption: `sfarrow::st_read_parquet()` or geoarrow produces sf object from downloaded pin
5. Geometry edge cases: ca_boundaries_bgc_tbl has only MULTIPOLYGON; lsoa_2021_lep_tbl has 2 rows where geom_valid=false
</verification>

<success_criteria>
All 8 spatial tables are accessible through both DuckLake (native GEOMETRY with spatial SQL) and pins (GeoParquet with spatial metadata). Edge cases (mixed types, invalid geometries) are handled per research recommendations. Analysts can query spatial data with ST_Contains/ST_Intersects in DuckLake and read GeoParquet pins in R sf and Python geopandas.
</success_criteria>

<output>
After completion, create `.planning/phases/04-spatial-data-handling/04-02-SUMMARY.md`
</output>
