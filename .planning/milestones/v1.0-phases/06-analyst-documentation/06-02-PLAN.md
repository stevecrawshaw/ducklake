---
phase: 06-analyst-documentation
plan: 02
type: execute
wave: 2
depends_on: [06-01]
files_modified:
  - docs/analyst-guide.qmd
autonomous: false
requirements: [INFRA-03, INFRA-04]
must_haves:
  truths:
    - "An analyst can follow the Prerequisites section and install all required R packages"
    - "An analyst can follow the AWS Credentials section and configure access (incorporates existing analyst-aws-setup.md content)"
    - "An analyst can follow the Pins section and read a dataset from S3 in R"
    - "An analyst can follow the DuckLake section and attach the catalogue and run SQL queries"
    - "An analyst can follow the Spatial section and read GeoParquet, set CRS, and plot a map"
    - "An analyst can follow the Python appendix and read data from S3 using Python pins"
    - "The Available Datasets section includes a summary table of all 18 datasets"
    - "The guide renders to both HTML and PDF without errors"
  artefacts:
    - path: "docs/analyst-guide.qmd"
      provides: "Complete analyst guide with all sections authored"
      min_lines: 500
  key_links:
    - from: "docs/analyst-guide.qmd"
      to: "scripts/test_interop.R"
      via: "Code patterns distilled from validated scripts"
      pattern: "board_s3|pin_read|pin_list"
    - from: "docs/analyst-guide.qmd"
      to: "scripts/verify_ducklake.sql"
      via: "DuckLake SQL patterns distilled from validated scripts"
      pattern: "ATTACH.*ducklake|INSTALL ducklake"
    - from: "docs/analyst-guide.qmd"
      to: "scripts/spike_spatial.R"
      via: "Spatial patterns distilled from validated scripts"
      pattern: "st_as_sf|st_set_crs|read_parquet"
---

<objective>
Author the complete analyst guide content — all sections from introduction through appendices.

Purpose: Fill the skeleton from Plan 01 with comprehensive, executable code examples and explanations so that an unfamiliar WECA analyst can go from zero to querying data within 10 minutes.

Output: A complete, renderable analyst-guide.qmd covering prerequisites, pins access, DuckLake queries, spatial data, troubleshooting, and Python appendix.
</objective>

<execution_context>
@C:/Users/steve.crawshaw/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/steve.crawshaw/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-analyst-documentation/06-RESEARCH.md
@.planning/phases/06-analyst-documentation/06-CONTEXT.md
@.planning/phases/06-analyst-documentation/06-01-SUMMARY.md

# Source scripts to distil code examples from:
@scripts/test_interop.R
@scripts/test_interop.py
@scripts/verify_ducklake.sql
@scripts/validate_pins_r.R
@scripts/validate_pins.py
@scripts/spike_spatial.R
@scripts/validate_ducklake.R
@docs/analyst-aws-setup.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Author Introduction, Prerequisites, and Available Datasets sections</name>
  <files>docs/analyst-guide.qmd</files>
  <action>
Replace the skeleton placeholders with full content for the first three sections:

**Introduction:**
- What this guide covers (pins, DuckLake, spatial data)
- The 10-minute promise: "you will be able to read your first dataset within 10 minutes"
- Brief overview of the two access paths: pins (data frames) vs DuckLake (SQL)
- When to use which: pins for quick data frame access, DuckLake for SQL queries/joins/views

**Prerequisites and Setup:**
- R packages subsection: `install.packages(c("pins", "arrow", "sf", "duckdb", "DBI"))` with brief explanation of each
- DuckDB installation subsection: Download link, verify with `duckdb --version`, note that DuckDB CLI is needed for DuckLake (R duckdb package v1.4.4 lacks ducklake extension — per project decision)
- AWS credentials subsection: Incorporate content from `docs/analyst-aws-setup.md` directly (Steps 1-3: credentials file, config file, verification). Adapt from markdown to Quarto style with callout blocks for warnings (region must be eu-west-2). Do NOT simply link to the external file — bring the content in so the guide is self-contained.

**Available Datasets (quick reference):**
- Static summary table of all 18 datasets with columns: Name, Description, Type (table/view), Rows, Spatial
- Source this from datasets_catalogue content (30 rows: 18 tables + 12 views)
- Include a note: "For full column-level details, query the datasets_catalogue and columns_catalogue programmatically (shown in Section 5.5)"
- Mark spatial tables clearly

All code examples should be R code chunks with `{r}` and `#| eval: false`. Show representative output in ``` text ``` blocks beneath each chunk.
  </action>
  <verify>
    <automated>quarto render docs/analyst-guide.qmd --to html 2>&amp;1 | tail -3</automated>
    <manual>Verify Introduction, Prerequisites, and Available Datasets sections have substantive content (not placeholders)</manual>
  </verify>
  <done>First three sections authored with install commands, AWS credential setup, and dataset summary table</done>
</task>

<task type="auto">
  <name>Task 2: Author Pins, DuckLake, Spatial, Troubleshooting, Support, and Appendices</name>
  <files>docs/analyst-guide.qmd</files>
  <action>
Replace remaining skeleton placeholders with full content:

**Accessing Data via Pins (R):**
- Board creation: `board_s3(bucket = "stevecrawshaw-bucket", prefix = "pins/", region = "eu-west-2", versioned = TRUE)` (from test_interop.R)
- `pin_list(board)` with representative output
- `pin_read(board, "ca_la_lookup_tbl")` with representative output (show first few rows)
- Metadata: `pin_meta(board, "ca_la_lookup_tbl")` showing title, user$columns
- Large datasets: Note that `raw_domestic_epc_certificates_tbl` (19M rows) uses chunked upload — `pin_read` works but may be slow. For large tables, suggest `arrow::open_dataset()` on the pin path.

**Querying the DuckLake Catalogue:**
- Installing extensions: `INSTALL ducklake; LOAD ducklake;` and `INSTALL spatial; LOAD spatial;`
- Creating S3 secret: `CREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain);`
- Attaching: `ATTACH 'ducklake:data/mca_env.ducklake' AS lake (READ_ONLY, DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/');`
- Important: Explain that analysts need the `data/mca_env.ducklake` file (clone the repo or download it). The file is small metadata; actual data lives on S3.
- Basic queries: SELECT, WHERE, GROUP BY examples using real tables
- Views: `SELECT * FROM lake.la_ghg_emissions_weca_vw` — explain WECA-filtered views
- Querying the catalogue: `SELECT name, description, type, row_count FROM lake.datasets_catalogue ORDER BY type, name;` and column catalogue query
- Time travel: Brief syntax example `SELECT COUNT(*) FROM lake.ca_la_lookup_tbl AT (VERSION => 1);` with one paragraph explanation

Use `{sql}` code chunks for SQL examples. Note that DuckLake SQL must be run in DuckDB CLI (not R duckdb package).

**Working with Spatial Data:**
- Reading GeoParquet: `pin_download(board, "bdline_ua_lep_tbl")` then `arrow::read_parquet()` then `sf::st_as_sf()` (from spike_spatial.R)
- WARNING callout: Do NOT use sfarrow — it fails on DuckDB GeoParquet (missing CRS metadata)
- Setting CRS: `sf::st_set_crs(sf_obj, 27700)` — MANDATORY because DuckDB does not embed CRS in GeoParquet. Most spatial tables use EPSG:27700 (British National Grid); check pin metadata for the correct CRS.
- Quick map: `plot(sf_obj["column_name"])` example

**Troubleshooting:**
- Dedicated section covering common issues (from 06-RESEARCH.md Common Pitfalls):
  1. "Access Denied" / region errors (eu-west-2 requirement)
  2. "File not found" for .ducklake (need to clone repo)
  3. CRS is NA or OGC:CRS84 (must set explicitly)
  4. sfarrow fails (use arrow + sf instead)
  5. Python board_s3 path format ("bucket/prefix" not separate params)
  6. Python pin_read fails on EPC table (multi-file pin — use arrow fallback)
  7. DuckLake extension not available in R (use DuckDB CLI)

**Support and Contact:**
- Placeholder for Slack channel: `[#data-platform]`
- Placeholder for contact: `[Data Platform Team]`
- Note: "Update these placeholders with your team's actual contact details before distributing"

**Appendix A: Python Equivalents:**
- Pins: board_s3("stevecrawshaw-bucket/pins", versioned=True), pin_list(), pin_read() (from test_interop.py)
- Highlight the path format difference: Python uses "bucket/prefix" not separate params
- DuckLake: import duckdb, con.execute("INSTALL ducklake; LOAD ducklake;"), attach, query
- Spatial: geopandas.read_parquet(), set_crs("EPSG:27700", allow_override=True) (from spike_spatial.R Python validation)

**Appendix B: SQL Quick Reference:**
- Brief SQL primer: SELECT, FROM, WHERE, GROUP BY, ORDER BY, JOIN, LIMIT
- Use DuckLake table examples (not abstract examples)
- Keep it concise — enough to get started, not a SQL course (per user discretion decision)
  </action>
  <verify>
    <automated>quarto render docs/analyst-guide.qmd --to html 2>&amp;1 | tail -3</automated>
    <manual>Verify all sections have substantive content, no placeholder comments remain, code examples match verified patterns from source scripts</manual>
  </verify>
  <done>All sections authored: Pins, DuckLake, Spatial, Troubleshooting, Support, Python appendix, SQL appendix. Guide is complete and renders to HTML.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify complete analyst guide</name>
  <what-built>Complete analyst guide (docs/analyst-guide.qmd) with all sections authored: Introduction, Prerequisites, Available Datasets, Pins Access, DuckLake Queries, Spatial Data, Troubleshooting, Support, Python Appendix, SQL Quick Reference. Renders to both HTML and PDF with WECA branding.</what-built>
  <how-to-verify>
    1. Open docs/analyst-guide.html in your browser
    2. Read through and verify: code examples look correct, flow is logical, no placeholder text remains
    3. Check: AWS credential section is complete and self-contained
    4. Check: Dataset summary table lists all 18 tables + views
    5. Check: Pins section shows board creation, pin_list, pin_read, pin_meta
    6. Check: DuckLake section shows attach, query, views, catalogue, time travel
    7. Check: Spatial section shows GeoParquet reading, CRS setting, plotting
    8. Check: Troubleshooting covers common pitfalls
    9. Check: Python appendix has equivalent examples
    10. Optionally render PDF: `quarto render docs/analyst-guide.qmd --to weca-report-typst`
    11. Assess: Could an unfamiliar analyst follow this and read data within 10 minutes?
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues</resume-signal>
</task>

</tasks>

<verification>
1. `quarto render docs/analyst-guide.qmd --to html` succeeds
2. HTML file contains all expected sections (no skeleton placeholders)
3. Code examples match patterns from validated source scripts
4. AWS credential setup is self-contained (not just a link to external file)
5. Dataset summary table lists all 18 tables
6. Spatial section includes CRS warning and correct workaround pattern
7. Python appendix covers pins, DuckLake, and spatial equivalents
8. Troubleshooting section covers all documented pitfalls
9. Support section has placeholders for contact info
</verification>

<success_criteria>
- Complete analyst guide rendering to HTML with WECA branding
- All code examples are distilled from validated project scripts (not invented)
- An unfamiliar analyst can follow the guide from setup to first data query
- No placeholder comments or skeleton text remains
- Renders to both HTML and PDF without errors
</success_criteria>

<output>
After completion, create `.planning/phases/06-analyst-documentation/06-02-SUMMARY.md`
</output>
