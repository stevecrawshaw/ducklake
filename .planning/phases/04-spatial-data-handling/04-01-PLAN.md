---
phase: 04-spatial-data-handling
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/spike_spatial.sql
  - scripts/spike_spatial.R
autonomous: true

must_haves:
  truths:
    - "bdline_ua_lep_diss_tbl exists in DuckLake with native GEOMETRY column (not BLOB)"
    - "Spatial SQL (ST_Area) works on the DuckLake GEOMETRY column"
    - "GeoParquet file exported from DuckDB contains valid geometry metadata"
    - "GeoParquet pin uploaded to S3 with spatial=TRUE metadata"
    - "R can download the pin, read as sf object, and access geometry"
    - "Python can download the pin, read as GeoDataFrame, and access geometry"
  artefacts:
    - path: "scripts/spike_spatial.sql"
      provides: "SQL for DuckLake GEOMETRY recreation and GeoParquet export"
    - path: "scripts/spike_spatial.R"
      provides: "R wrapper for spike: DuckDB CLI execution, pin_upload, R/Python validation"
  key_links:
    - from: "scripts/spike_spatial.sql"
      to: "data/mca_env.ducklake"
      via: "DROP + CREATE TABLE with ST_GeomFromWKB()"
    - from: "scripts/spike_spatial.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "pin_upload of GeoParquet file"
---

<objective>
Spike the full spatial pipeline with one small table (bdline_ua_lep_diss_tbl, 1 row) to validate:
DuckLake native GEOMETRY creation, spatial SQL, GeoParquet export, pin_upload, and R/Python roundtrip consumption.

Purpose: De-risk the spatial conversion before committing to all 8 tables. This validates every link in the chain with the simplest possible table.
Output: Working spike scripts and a validated spatial pin on S3.
</objective>

<execution_context>
@C:\Users\steve.crawshaw\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\steve.crawshaw\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-spatial-data-handling/04-RESEARCH.md
@scripts/create_ducklake.R
@scripts/create_ducklake.sql
@scripts/export_pins.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: DuckLake GEOMETRY recreation and GeoParquet export spike</name>
  <files>scripts/spike_spatial.sql, scripts/spike_spatial.R</files>
  <action>
Create a SQL script `scripts/spike_spatial.sql` that:
1. Installs and loads extensions: ducklake, httpfs, aws, spatial
2. Creates S3 credential (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain)
3. Attaches the DuckLake catalogue: `ATTACH 'ducklake:data/mca_env.ducklake' AS lake (DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/')`
4. Attaches the source database: `ATTACH 'data/mca_env_base.duckdb' AS source (READ_ONLY)`
5. Drops the existing BLOB-typed table: `DROP TABLE IF EXISTS lake.bdline_ua_lep_diss_tbl`
6. Recreates with native GEOMETRY: `CREATE TABLE lake.bdline_ua_lep_diss_tbl AS SELECT * EXCLUDE(shape), ST_GeomFromWKB(shape) AS shape FROM source.bdline_ua_lep_diss_tbl`
7. Verifies GEOMETRY type: query `typeof(shape)` to confirm it is GEOMETRY, not BLOB
8. Runs a spatial SQL test: `SELECT ST_Area(shape) AS area_sqm FROM lake.bdline_ua_lep_diss_tbl` -- should return a non-zero value
9. Exports GeoParquet to a temp file: `COPY (SELECT * FROM lake.bdline_ua_lep_diss_tbl) TO 'data/tmp_spike_spatial.parquet' (FORMAT PARQUET)`
10. Verifies GeoParquet metadata by reading back: `SELECT * FROM parquet_metadata('data/tmp_spike_spatial.parquet')` -- look for geo key in key_value_metadata

Create an R wrapper `scripts/spike_spatial.R` that:
1. Executes the SQL via DuckDB CLI (same pattern as create_ducklake.R: write SQL to temp file, run `duckdb -init`)
2. After SQL succeeds, uploads the GeoParquet file as a pin:
   ```r
   board <- board_s3(bucket = "stevecrawshaw-bucket", prefix = "pins/", region = "eu-west-2", versioned = TRUE)
   pin_upload(board, paths = "data/tmp_spike_spatial.parquet", name = "bdline_ua_lep_diss_tbl",
     title = "Boundary line UA LEP dissolved",
     description = "Boundary line UA LEP dissolved (1 row, 3 columns, GeoParquet, EPSG:27700)",
     metadata = list(source_db = "ducklake", spatial = TRUE, geometry_column = "shape",
       geometry_type = "POLYGON", crs = "EPSG:27700"))
   ```
   NOTE: This pin name matches the existing non-spatial pin from Phase 2. pin_upload with versioned=TRUE will create a new version, replacing the old non-spatial version. This is the intended behaviour -- the spatial GeoParquet version supersedes the Phase 2 BLOB export.
3. Validates R roundtrip:
   - `pin_download(board, "bdline_ua_lep_diss_tbl")` to get the file path
   - Try `sfarrow::st_read_parquet(path)` first. If sfarrow fails, try `arrow::read_parquet(path, as_data_frame = FALSE) |> sf::st_as_sf()`. Log which method works.
   - Confirm the result is an sf object with a geometry column. Print `class()`, `sf::st_geometry_type()`, `sf::st_crs()`, and `nrow()`.
   - Note: CRS may be NA (known gap from research -- DuckDB COPY TO does not write CRS into GeoParquet metadata). If NA, set it: `sf::st_set_crs(sf_obj, 27700)` and confirm it works. Log the CRS status.
4. Validates Python roundtrip (via `system()` calling Python):
   ```python
   import pins
   import geopandas as gpd
   board = pins.board_s3("stevecrawshaw-bucket", prefix="pins/", region="eu-west-2")
   path = board.pin_download("bdline_ua_lep_diss_tbl")
   gdf = gpd.read_parquet(path[0])
   print(f"Type: {type(gdf)}")
   print(f"Geometry type: {gdf.geometry.geom_type.unique()}")
   print(f"CRS: {gdf.crs}")
   print(f"Rows: {len(gdf)}")
   ```
   Run this as a short Python script via `system("python scripts/.tmp_validate_spatial.py")`. Write the Python script from R, execute, capture output, clean up.
5. Cleans up `data/tmp_spike_spatial.parquet` temp file after upload.
6. Prints a summary of all validation results: DuckLake GEOMETRY type confirmed, spatial SQL works, GeoParquet pin uploaded, R roundtrip result, Python roundtrip result.

Key patterns to follow:
- DuckDB CLI execution pattern from `scripts/create_ducklake.R` (write SQL to temp file, execute with `-init`, capture output)
- Pin upload pattern from `scripts/export_pins.R` (board_s3, pin_upload with metadata)
- Use `library(pins)` for R pin operations
- Use `uv run python` instead of bare `python` for Python execution (project uses uv)
- Handle errors gracefully: if any step fails, log the error clearly and continue to the next validation step
  </action>
  <verify>
Run `Rscript scripts/spike_spatial.R` from the project root. Expected output:
1. DuckDB CLI completes without errors
2. GEOMETRY type confirmed (not BLOB)
3. ST_Area returns a non-zero numeric value
4. GeoParquet file exported and uploaded as pin
5. R reads the pin as sf object with geometry column
6. Python reads the pin as GeoDataFrame with geometry column
7. Summary shows all steps passed
  </verify>
  <done>
- bdline_ua_lep_diss_tbl exists in DuckLake with GEOMETRY column type
- Spatial SQL (ST_Area) returns a valid result on the DuckLake table
- GeoParquet pin exists on S3 at pins/bdline_ua_lep_diss_tbl with spatial=TRUE metadata
- R sf roundtrip succeeds (sf object with geometry column)
- Python geopandas roundtrip succeeds (GeoDataFrame with geometry column)
- Spike scripts exist at scripts/spike_spatial.sql and scripts/spike_spatial.R
  </done>
</task>

</tasks>

<verification>
After running `Rscript scripts/spike_spatial.R`:
1. DuckLake table check: `duckdb -c "INSTALL ducklake; LOAD ducklake; INSTALL spatial; LOAD spatial; INSTALL httpfs; LOAD httpfs; INSTALL aws; LOAD aws; CREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain); ATTACH 'ducklake:data/mca_env.ducklake' AS lake (DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/', READ_ONLY); SELECT typeof(shape) FROM lake.bdline_ua_lep_diss_tbl LIMIT 1;"` returns GEOMETRY
2. Pin exists: In R, `pin_meta(board, "bdline_ua_lep_diss_tbl")$user$spatial` returns TRUE
3. R consumption: `sfarrow::st_read_parquet()` or `arrow::read_parquet() |> sf::st_as_sf()` produces an sf object
4. Python consumption: `geopandas.read_parquet()` produces a GeoDataFrame
</verification>

<success_criteria>
The full spatial pipeline is validated end-to-end with one table: WKB_BLOB source -> DuckLake GEOMETRY -> GeoParquet pin -> R sf / Python geopandas consumption. All 6 truths in must_haves are confirmed. The approach is proven safe to scale to all 8 tables in plan 04-02.
</success_criteria>

<output>
After completion, create `.planning/phases/04-spatial-data-handling/04-01-SUMMARY.md`
</output>
