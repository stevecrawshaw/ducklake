---
phase: 02-table-export-via-pins
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/extract_metadata.R
  - scripts/test_interop.R
  - scripts/test_interop.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Metadata extraction script connects to source DuckDB and lists all tables with comments"
    - "Spatial tables are correctly identified and excluded from the non-spatial export list"
    - "A single test pin written from R can be read from Python with metadata intact"
    - "Python pins[aws] dependency is installed and working"
  artefacts:
    - path: "scripts/extract_metadata.R"
      provides: "DuckDB metadata extraction and spatial table identification"
      contains: "duckdb_tables"
    - path: "scripts/test_interop.R"
      provides: "Write a single test pin to S3 with custom metadata"
      contains: "pin_write"
    - path: "scripts/test_interop.py"
      provides: "Read the test pin from S3, verify metadata round-trip"
      contains: "board_s3"
  key_links:
    - from: "scripts/test_interop.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "board_s3 with prefix pins/"
      pattern: 'board_s3.*prefix.*pins'
    - from: "scripts/test_interop.py"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "board_s3 with bucket/prefix path"
      pattern: 'board_s3.*stevecrawshaw-bucket/pins'
---

<objective>
Extract metadata from the source DuckDB, identify non-spatial tables, and validate cross-language pins interoperability with a single test pin.

Purpose: This plan addresses the LOW-confidence cross-language interop risk identified in research. By writing one pin from R and reading it from Python (including custom metadata), we validate the entire pipeline before committing to a bulk export. It also produces the authoritative table list and metadata that Plan 02-02 needs.

Output: Metadata extraction script, interop test scripts (R write + Python read), confirmed table list, Python pins[aws] dependency installed.
</objective>

<execution_context>
@C:\Users\steve.crawshaw\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\steve.crawshaw\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-table-export-via-pins/02-RESEARCH.md
@aws_setup.r
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Metadata extraction and spatial table identification</name>
  <files>scripts/extract_metadata.R</files>
  <action>
Create an R script that connects to `data/mca_env_base.duckdb` (read-only) and:

1. Queries `duckdb_tables()` to get all non-internal tables in the `main` schema with their comments, estimated_size, and column_count.
2. Queries `duckdb_columns()` to get all column metadata (column_name, data_type, comment) for every table.
3. Identifies spatial tables by filtering columns where `data_type ILIKE '%BLOB%' OR data_type ILIKE '%GEOMETRY%' OR data_type ILIKE '%WKB%'`.
4. Prints a summary: total tables, spatial tables (excluded), non-spatial tables (to export), and for each non-spatial table: name, comment, row count, column count.
5. Saves the non-spatial table list and column metadata as an RDS file at `data/table_metadata.rds` (a named list with elements `tables` and `columns`, both data frames).

Use `library(duckdb)` and `library(DBI)`. Do NOT use pacman. The script should be runnable with `Rscript scripts/extract_metadata.R` and print results to stdout.

Important: The source DuckDB path should be a variable at the top of the script (`SOURCE_DB <- "data/mca_env_base.duckdb"`) so it is easy to change.
  </action>
  <verify>
Run `Rscript scripts/extract_metadata.R` and confirm:
- It prints a table summary with table names, comments, and row counts
- It identifies which tables are spatial (excluded) vs non-spatial (to export)
- `data/table_metadata.rds` is created
  </verify>
  <done>Script runs without error, prints the full table inventory, spatial tables are identified and excluded, metadata RDS file exists.</done>
</task>

<task type="auto">
  <name>Task 2: Cross-language interop validation (R write, Python read)</name>
  <files>scripts/test_interop.R, scripts/test_interop.py, pyproject.toml</files>
  <action>
**Step A: Update Python dependencies.**

In `pyproject.toml`, change `"pins>=0.9.1"` to `"pins[aws]>=0.9.1"` so that `s3fs` and `fsspec` are installed. Then run `uv sync` to install.

**Step B: Create R interop test script (`scripts/test_interop.R`).**

This script:
1. Connects to `data/mca_env_base.duckdb` (read-only).
2. Picks the SMALLEST non-spatial table (fewest rows) for a fast test.
3. Reads that table into a data frame.
4. Gets its table comment and column comments from DuckDB metadata.
5. Creates an S3 board: `board_s3(bucket = "stevecrawshaw-bucket", prefix = "pins/", region = "eu-west-2", versioned = TRUE)`.
6. Writes the pin with `pin_write()` using `type = "parquet"`, setting `title` to the table comment, `description` to the table comment, and `metadata = list(source_db = "ducklake", columns = <named list of column comments>, column_types = <named list of column data types>)`.
7. Immediately reads the pin back with `pin_read()` and verifies the data frame dimensions match.
8. Reads metadata with `pin_meta()` and prints the custom metadata (`$user`).
9. Prints "R INTEROP TEST PASSED" on success.

Use `library(pins)`, `library(duckdb)`, `library(DBI)`, `library(arrow)`. Do NOT use pacman.

**Step C: Create Python interop test script (`scripts/test_interop.py`).**

This script:
1. Sets `AWS_DEFAULT_REGION=eu-west-2` via `os.environ.setdefault`.
2. Creates board: `board_s3("stevecrawshaw-bucket/pins", versioned=True)`.
3. Lists all pins with `board.pin_list()` and prints them.
4. Reads the same test pin written by R (get pin name from command line arg or hardcode after R script runs -- better to list pins and pick the first one).
5. Prints the DataFrame shape, dtypes, and first 3 rows.
6. Reads metadata with `board.pin_meta(pin_name)` and prints `meta.title`, `meta.description`, and `meta.user`.
7. Validates that `meta.user` contains `columns` dict with at least one entry.
8. Prints "PYTHON INTEROP TEST PASSED" on success.

Use `from pins import board_s3`. Handle import errors for `s3fs` gracefully with a message pointing to `pins[aws]`.
  </action>
  <verify>
1. Run `Rscript scripts/test_interop.R` -- should print "R INTEROP TEST PASSED"
2. Run `uv run python scripts/test_interop.py` -- should print "PYTHON INTEROP TEST PASSED"
3. Both scripts should show the same pin name, matching row/column counts, and the custom metadata (column descriptions) should be readable from Python.
  </verify>
  <done>A pin written from R is successfully read from Python. Custom metadata (table description, column descriptions) survives the round-trip. The cross-language interop risk is resolved.</done>
</task>

</tasks>

<verification>
1. `Rscript scripts/extract_metadata.R` runs and prints table inventory
2. `data/table_metadata.rds` exists with tables and columns data frames
3. `Rscript scripts/test_interop.R` writes a test pin and prints "R INTEROP TEST PASSED"
4. `uv run python scripts/test_interop.py` reads the test pin and prints "PYTHON INTEROP TEST PASSED"
5. Metadata round-trip confirmed (column descriptions visible in Python)
</verification>

<success_criteria>
- Non-spatial table list is known and documented
- Spatial tables are identified for Phase 4 deferral
- Cross-language pins interop is validated (R write -> Python read)
- Custom metadata (table/column descriptions) survives round-trip
- Python pins[aws] dependency is installed and working
</success_criteria>

<output>
After completion, create `.planning/phases/02-table-export-via-pins/02-01-SUMMARY.md`
</output>
