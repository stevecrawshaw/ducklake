---
phase: 02-table-export-via-pins
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - scripts/validate_pins_r.R
  - scripts/validate_pins.py
autonomous: true

must_haves:
  truths:
    - "An analyst in R can run pin_list(board) and see all exported datasets"
    - "An analyst in R can run pin_read(board, name) and get a correct data frame"
    - "An analyst in Python can list all pins from the same S3 board"
    - "An analyst in Python can read any pin and get a correct DataFrame"
    - "Pin metadata (table description, column descriptions) is accessible from both languages"
  artefacts:
    - path: "scripts/validate_pins_r.R"
      provides: "R validation of all pins: list, read, metadata check"
      contains: "pin_read"
    - path: "scripts/validate_pins.py"
      provides: "Python validation of all pins: list, read, metadata check"
      contains: "pin_read"
  key_links:
    - from: "scripts/validate_pins_r.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "board_s3 read operations"
      pattern: 'pin_list|pin_read|pin_meta'
    - from: "scripts/validate_pins.py"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "board_s3 read operations"
      pattern: 'pin_list|pin_read|pin_meta'
---

<objective>
Validate that all exported pins are readable from both R and Python, with correct data types and metadata.

Purpose: This is the acceptance test for Phase 2. It proves the six success criteria from the roadmap: analysts can discover, read, and understand datasets from both R and Python. It also confirms the cross-language interop at scale (not just one test pin).

Output: Validation scripts for R and Python, pass/fail report for every pin.
</objective>

<execution_context>
@C:\Users\steve.crawshaw\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\steve.crawshaw\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-table-export-via-pins/02-RESEARCH.md
@.planning/phases/02-table-export-via-pins/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: R validation script</name>
  <files>scripts/validate_pins_r.R</files>
  <action>
Create `scripts/validate_pins_r.R` that systematically validates all pins from R.

**Libraries:** `library(pins)`, `library(arrow)`. Do NOT use pacman.

**Logic:**

1. Create board: `board_s3(bucket = "stevecrawshaw-bucket", prefix = "pins/", region = "eu-west-2", versioned = TRUE)`.
2. Get pin list: `all_pins <- pin_list(board)`.
3. Print total pin count.
4. For each pin:
   a. Read with `pin_read(board, pin_name)` -- confirm it returns a data frame with >0 rows and >0 columns.
   b. Read metadata with `pin_meta(board, pin_name)`.
   c. Check `meta$title` is not NULL/NA/empty.
   d. Check `meta$user$columns` is a named list with at least one entry.
   e. Print: `"PASS: {pin_name} ({nrow} rows x {ncol} cols, title: {title})"` or `"FAIL: {pin_name} -- {reason}"`.
5. Print summary: `"{passed}/{total} pins passed validation"`.
6. Exit with code 0 if all passed, code 1 if any failed.

**Error handling:** Wrap each pin check in `tryCatch`. A pin that errors during read counts as FAIL.

**Important:** Do NOT read the entire 19M-row EPC table into memory just for validation. For tables with very many rows, read only the first few rows if possible, or just confirm `pin_read` succeeds and check `nrow > 0`. The pins package reads the full parquet by default, so for the EPC pin, use `arrow::read_parquet(pin_download(board, pin_name), as_data_frame = FALSE)` and check `$num_rows` from the Arrow Table without loading into R memory. Then take `head(df, 5)` for the type check.
  </action>
  <verify>
Run `Rscript scripts/validate_pins_r.R` and confirm all pins pass.
  </verify>
  <done>All pins are readable from R with correct metadata. Script exits with code 0.</done>
</task>

<task type="auto">
  <name>Task 2: Python validation script</name>
  <files>scripts/validate_pins.py</files>
  <action>
Create `scripts/validate_pins.py` that validates all pins from Python.

**Logic:**

1. Set `os.environ.setdefault("AWS_DEFAULT_REGION", "eu-west-2")`.
2. Create board: `board_s3("stevecrawshaw-bucket/pins", versioned=True)`.
3. List all pins: `all_pins = board.pin_list()`.
4. Print total pin count.
5. For each pin:
   a. Read with `board.pin_read(pin_name)` -- confirm it returns a DataFrame with >0 rows and >0 columns.
   b. Read metadata with `board.pin_meta(pin_name)`.
   c. Check `meta.title` is not None/empty.
   d. Check `meta.user` contains a `columns` dict with at least one entry.
   e. Print: `"PASS: {pin_name} ({rows} rows x {cols} cols, title: {title})"` or `"FAIL: {pin_name} -- {reason}"`.
6. Print summary: `"{passed}/{total} pins passed validation"`.
7. Exit with code 0 if all passed, code 1 if any failed.

**Error handling:** Wrap each pin in try/except. A pin that errors counts as FAIL.

**Large table handling:** For pins with very large data, `pin_read` returns a pandas DataFrame which loads into memory. If the EPC pin causes OOM in Python, catch the MemoryError specifically and note it as a known limitation (Python analysts may need to use DuckDB/arrow directly for that table). This is acceptable -- document the finding but do not count it as a validation failure if all other pins pass and the pin is at least listable and has correct metadata.

**Import error handling:** If `s3fs` is not installed, print a clear error: "Missing s3fs. Run: uv add 'pins[aws]>=0.9.1'" and exit with code 2.
  </action>
  <verify>
Run `uv run python scripts/validate_pins.py` and confirm all pins pass (or all pass except a documented large-table memory note).
  </verify>
  <done>All pins are readable from Python with correct metadata. Pin list matches R. Script exits with code 0.</done>
</task>

</tasks>

<verification>
1. `Rscript scripts/validate_pins_r.R` -- all pins PASS, exit code 0
2. `uv run python scripts/validate_pins.py` -- all pins PASS, exit code 0
3. Pin count matches between R and Python
4. Metadata (title, column descriptions) accessible from both languages
5. Both scripts handle the large EPC table without crashing
</verification>

<success_criteria>
- All non-spatial tables readable from R via pin_read
- All non-spatial tables readable from Python via pin_read
- Pin metadata (table description, column descriptions) accessible from both languages
- Pin list identical between R and Python
- Large EPC table handled gracefully in both languages
</success_criteria>

<output>
After completion, create `.planning/phases/02-table-export-via-pins/02-03-SUMMARY.md`
</output>
