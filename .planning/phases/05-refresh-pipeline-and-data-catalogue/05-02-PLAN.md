---
phase: 05-refresh-pipeline-and-data-catalogue
plan: 02
type: execute
wave: 2
depends_on:
  - "05-01"
files_modified:
  - scripts/refresh.R
autonomous: true
requirements:
  - CAT-01
  - CAT-02
  - CAT-03
must_haves:
  truths:
    - "After refresh completes, a datasets_catalogue table exists in DuckLake listing all 18 base tables and 12 views with descriptions, types, row counts, and last updated dates"
    - "After refresh completes, a columns_catalogue table exists in DuckLake listing every column across all base tables with name, type, description, and up to 3 example values"
    - "Spatial tables in datasets_catalogue include geometry_type, crs, and bounding box columns"
    - "Both catalogue tables are also pinned to S3 as parquet files accessible from R and Python"
    - "Catalogue is regenerated automatically at the end of every refresh run"
  artefacts:
    - path: "scripts/refresh.R"
      provides: "Catalogue generation integrated at end of refresh pipeline"
      contains: "datasets_catalogue"
  key_links:
    - from: "scripts/refresh.R"
      to: "lake.datasets_catalogue"
      via: "CREATE OR REPLACE TABLE via DuckDB CLI"
      pattern: "datasets_catalogue"
    - from: "scripts/refresh.R"
      to: "lake.columns_catalogue"
      via: "CREATE OR REPLACE TABLE via DuckDB CLI"
      pattern: "columns_catalogue"
    - from: "scripts/refresh.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "pin_write for catalogue parquet files"
      pattern: "pin_write.*catalogue"
---

<objective>
Extend `scripts/refresh.R` to generate two catalogue tables (datasets_catalogue and columns_catalogue) as the final step of every refresh run, then pin them to S3. The catalogue lets analysts discover available datasets, understand column structures, and see example values — all without asking the data owner.

Purpose: Analysts can query `SELECT * FROM lake.datasets_catalogue` or read the pinned parquet to find out what data exists, what columns are available, and what the data looks like.
Output: Extended `scripts/refresh.R` with catalogue generation; two new DuckLake tables and S3 pins.
</objective>

<execution_context>
@C:/Users/steve.crawshaw/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/steve.crawshaw/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-refresh-pipeline-and-data-catalogue/05-CONTEXT.md
@.planning/phases/05-refresh-pipeline-and-data-catalogue/05-RESEARCH.md
@.planning/phases/05-refresh-pipeline-and-data-catalogue/05-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add datasets_catalogue generation to refresh.R</name>
  <files>scripts/refresh.R</files>
  <action>
Add a catalogue generation section to the end of `scripts/refresh.R`, after the main refresh loop and summary table. This section creates the `datasets_catalogue` table in DuckLake and pins it to S3.

**datasets_catalogue schema:**
| Column | Type | Source |
|--------|------|--------|
| name | VARCHAR | table_name from duckdb_tables() / duckdb_views() |
| description | VARCHAR | comment from duckdb_tables(); for views, generate from name pattern (e.g. "WECA-filtered subset of {base_table}" for _weca_ views, or descriptive text for other views) |
| type | VARCHAR | 'table' or 'view' |
| row_count | BIGINT | from results data.frame (already computed during refresh) for tables; COUNT(*) via DuckDB CLI for views |
| last_updated | TIMESTAMP | Sys.time() at script start (same for all entries in a run) |
| source_table | VARCHAR | table_name for tables; base table name extracted from view definition for views |
| geometry_type | VARCHAR | from spatial_tables metadata (NULL for non-spatial) |
| crs | VARCHAR | from spatial_tables metadata (NULL for non-spatial) |
| bbox_xmin | DOUBLE | computed via ST_XMin(ST_Extent(geom_col)) for spatial tables (NULL for non-spatial) |
| bbox_ymin | DOUBLE | computed via ST_YMin(ST_Extent(geom_col)) for spatial tables (NULL for non-spatial) |
| bbox_xmax | DOUBLE | computed via ST_XMax(ST_Extent(geom_col)) for spatial tables (NULL for non-spatial) |
| bbox_ymax | DOUBLE | computed via ST_YMax(ST_Extent(geom_col)) for spatial tables (NULL for non-spatial) |

**Implementation approach:**
1. Build the catalogue data.frame in R using data already available from the refresh loop (table names, row counts, spatial metadata)
2. For the 12 views: query `duckdb_views() WHERE database_name = 'lake'` and get row counts via `SELECT COUNT(*)` for each view, all via DuckDB CLI
3. For spatial bounding boxes: build SQL that queries `ST_Extent()` for each spatial table via DuckDB CLI, parse the output
4. For view descriptions: generate programmatically based on view name patterns:
   - Views ending in `_weca_vw`: "WECA-filtered subset of {base_table_name}"
   - `ca_la_lookup_inc_ns_vw`: "CA/LA lookup including North Somerset"
   - `weca_lep_la_vw`: "WECA LEP local authorities"
   - `ca_la_ghg_emissions_sub_sector_ods_vw`: "GHG emissions by sub-sector with CA/LA lookup"
   - `epc_domestic_vw`: "Domestic EPC certificates with derived fields"
5. Write the data.frame to a temp CSV, then use DuckDB CLI to `CREATE OR REPLACE TABLE lake.datasets_catalogue AS SELECT * FROM read_csv('{tmp_csv}')` — this avoids complex SQL string building for 30 rows
6. Pin the catalogue to S3 via `pin_write(board, datasets_df, name = "datasets_catalogue", type = "parquet")`

**Include both base tables (18) and views (12) = 30 rows total.** A `type` column distinguishes them.

**Do NOT include datasets_catalogue and columns_catalogue themselves** in the catalogue (they are metadata, not data).
  </action>
  <verify>
    <automated>Rscript -e "library(pins); b <- board_s3('stevecrawshaw-bucket', prefix='pins/', region='eu-west-2', versioned=TRUE); d <- pin_read(b, 'datasets_catalogue'); cat(sprintf('rows=%d cols=%d types=%s\n', nrow(d), ncol(d), paste(unique(d[['type']]), collapse=','))); cat(sprintf('spatial_with_bbox=%d\n', sum(!is.na(d[['bbox_xmin']]))))"</automated>
    <manual>Verify datasets_catalogue has 30 rows (18 tables + 12 views), spatial entries have bounding boxes, descriptions are populated</manual>
    <sampling_rate>run after this task commits</sampling_rate>
  </verify>
  <done>
- datasets_catalogue exists as a DuckLake table with 30 rows (18 tables + 12 views)
- Each row has name, description, type, row_count, last_updated, source_table
- Spatial entries additionally have geometry_type, crs, bbox_xmin/ymin/xmax/ymax
- datasets_catalogue is pinned to S3 as parquet
  </done>
</task>

<task type="auto">
  <name>Task 2: Add columns_catalogue generation to refresh.R</name>
  <files>scripts/refresh.R</files>
  <action>
Extend the catalogue section of `scripts/refresh.R` to also generate a `columns_catalogue` table.

**columns_catalogue schema:**
| Column | Type | Source |
|--------|------|--------|
| table_name | VARCHAR | from duckdb_columns() |
| column_name | VARCHAR | from duckdb_columns() |
| data_type | VARCHAR | from duckdb_columns() |
| description | VARCHAR | comment from duckdb_columns() |
| example_1 | VARCHAR | first distinct non-null value (cast to VARCHAR) |
| example_2 | VARCHAR | second distinct non-null value (cast to VARCHAR) |
| example_3 | VARCHAR | third distinct non-null value (cast to VARCHAR) |

**Implementation approach:**
1. Query `duckdb_columns() WHERE database_name = 'lake'` via DuckDB CLI to get all columns for base tables (exclude catalogue tables and views — views share columns with their base tables so including them would be redundant)
2. For example values, build SQL that samples from each base table. Use the efficient pattern from research:
   ```sql
   SELECT DISTINCT CAST({col} AS VARCHAR) AS val
   FROM (SELECT {col} FROM lake.{table} WHERE {col} IS NOT NULL LIMIT 1000)
   LIMIT 3
   ```
3. Execute example sampling via DuckDB CLI in batches (one SQL file per table, not per column — reduces CLI invocations from ~hundreds to 18)
4. Parse CLI output to extract example values
5. Build columns_catalogue data.frame in R
6. Write to DuckLake via temp CSV + `CREATE OR REPLACE TABLE lake.columns_catalogue AS SELECT * FROM read_csv('{tmp_csv}')`
7. Pin to S3 via `pin_write(board, columns_df, name = "columns_catalogue", type = "parquet")`

**Important considerations:**
- GEOMETRY columns: cast example values with `ST_AsText({col})` instead of plain CAST (geometry cannot be cast to VARCHAR directly)
- BLOB columns: skip example values (set to NULL) — binary data is not meaningful as text
- Only sample from base tables (18), not views — avoids duplicate effort and slow view scans
- The EPC table has ~80 columns and 19M rows — the `LIMIT 1000` subquery pattern keeps sampling fast
- Exclude datasets_catalogue and columns_catalogue themselves from the column listing
  </action>
  <verify>
    <automated>Rscript -e "library(pins); b <- board_s3('stevecrawshaw-bucket', prefix='pins/', region='eu-west-2', versioned=TRUE); c <- pin_read(b, 'columns_catalogue'); cat(sprintf('rows=%d tables=%d has_examples=%d\n', nrow(c), length(unique(c[['table_name']])), sum(!is.na(c[['example_1']]))))"</automated>
    <manual>Verify columns_catalogue covers all 18 base tables, descriptions populated for columns that have comments, example values present for most non-geometry columns</manual>
    <sampling_rate>run after this task commits</sampling_rate>
  </verify>
  <done>
- columns_catalogue exists as a DuckLake table with one row per column across all 18 base tables
- Each row has table_name, column_name, data_type, description
- Example values populated for most columns (NULL for BLOB/GEOMETRY types)
- columns_catalogue is pinned to S3 as parquet
- Full refresh (`Rscript scripts/refresh.R`) runs end-to-end including catalogue generation
  </done>
</task>

</tasks>

<verification>
1. Full refresh runs end-to-end: `Rscript scripts/refresh.R` completes with 18 tables refreshed + catalogue generated
2. datasets_catalogue queryable via DuckLake: `SELECT name, type, row_count FROM lake.datasets_catalogue ORDER BY type, name` returns 30 rows
3. columns_catalogue queryable via DuckLake: `SELECT COUNT(*) FROM lake.columns_catalogue` returns total column count across 18 tables
4. Both catalogues available as pins on S3
5. Spatial entries in datasets_catalogue have non-null bounding boxes
6. Example values in columns_catalogue are populated for non-binary columns
</verification>

<success_criteria>
- Analysts can run `SELECT * FROM lake.datasets_catalogue` to discover all available datasets
- Analysts can run `SELECT * FROM lake.columns_catalogue WHERE table_name = 'some_table'` to understand a table's structure
- Catalogue includes descriptions, row counts, column types, and example values
- Catalogue is automatically regenerated every time the refresh pipeline runs
</success_criteria>

<output>
After completion, create `.planning/phases/05-refresh-pipeline-and-data-catalogue/05-02-SUMMARY.md`
</output>
