---
phase: 05-refresh-pipeline-and-data-catalogue
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/refresh.R
autonomous: true
requirements:
  - REFRESH-01
  - REFRESH-02
  - REFRESH-03
must_haves:
  truths:
    - "Running `Rscript scripts/refresh.R` re-exports all 18 tables from source DuckDB to both DuckLake and S3 pins"
    - "After refresh, DuckLake tables contain current data with GEOMETRY columns for spatial tables"
    - "After refresh, pins on S3 are updated with new versions (parquet for non-spatial, GeoParquet for spatial)"
    - "Row counts match between source and destination for every table"
    - "Console summary shows table name, row count, time taken, and pass/fail for each table"
  artefacts:
    - path: "scripts/refresh.R"
      provides: "Single-command refresh pipeline for all 18 tables"
      min_lines: 200
  key_links:
    - from: "scripts/refresh.R"
      to: "data/mca_env_base.duckdb"
      via: "duckdb R package for source reads, DuckDB CLI for DuckLake writes"
      pattern: "dbConnect.*mca_env_base"
    - from: "scripts/refresh.R"
      to: "s3://stevecrawshaw-bucket/pins/"
      via: "pins::pin_write and pins::pin_upload"
      pattern: "pin_write|pin_upload"
    - from: "scripts/refresh.R"
      to: "data/mca_env.ducklake"
      via: "DuckDB CLI with ducklake extension"
      pattern: "mca_env\\.ducklake"
---

<objective>
Create `scripts/refresh.R` — a single R script entry point that re-exports all 18 tables from the source DuckDB to both DuckLake (DROP + CREATE TABLE) and S3 pins (pin_write/pin_upload). The script detects spatial vs non-spatial tables, applies per-table edge case handling (ST_Multi for ca_boundaries_bgc_tbl, geom_valid flag for lsoa_2021_lep_tbl, chunked upload for the 19M-row EPC table), validates row counts after each export, and prints a console summary table.

Purpose: The data owner can refresh all data with a single command (`Rscript scripts/refresh.R`), replacing the current multi-script workflow.
Output: `scripts/refresh.R` — fully functional refresh pipeline.
</objective>

<execution_context>
@C:/Users/steve.crawshaw/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/steve.crawshaw/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-refresh-pipeline-and-data-catalogue/05-CONTEXT.md
@.planning/phases/05-refresh-pipeline-and-data-catalogue/05-RESEARCH.md

# Existing scripts to consolidate (read for patterns, do NOT call from refresh.R):
@scripts/export_pins.R
@scripts/export_spatial_pins.R
@scripts/recreate_spatial_ducklake.sql
@scripts/create_ducklake.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create the unified refresh pipeline script</name>
  <files>scripts/refresh.R</files>
  <action>
Create `scripts/refresh.R` that consolidates the existing multi-script workflow into a single entry point. The script must:

**Configuration section:**
- SOURCE_DB = "data/mca_env_base.duckdb"
- DUCKLAKE_FILE = "data/mca_env.ducklake"
- DATA_PATH = "s3://stevecrawshaw-bucket/ducklake/data/"
- S3_BUCKET = "stevecrawshaw-bucket"
- S3_PREFIX = "pins/"
- S3_REGION = "eu-west-2"
- LARGE_TABLE_THRESHOLD = 5000000
- CHUNK_SIZE = 3000000

**Libraries:** pins, duckdb, DBI, arrow

**Spatial table metadata** (hardcoded data.frame, same pattern as export_spatial_pins.R):
- 8 spatial tables with geom_col, geom_type, crs columns
- Used for routing spatial export and pin metadata

**Spatial detection:** Use `grepl("BLOB|GEOMETRY|WKB", data_type, ignore.case = TRUE)` on `duckdb_columns()` to classify tables, matching existing pattern from export_pins.R.

**Main loop** over all 18 tables (from `duckdb_tables()` WHERE schema_name = 'main' AND internal = false):

For each table, do two things in sequence:

**(A) DuckLake export** (via DuckDB CLI):
- Build SQL: INSTALL/LOAD extensions (ducklake, httpfs, aws, spatial), CREATE SECRET, ATTACH ducklake catalogue, ATTACH source DB
- `DROP TABLE IF EXISTS lake.{table_name};`
- For non-spatial: `CREATE TABLE lake.{table_name} AS SELECT * FROM source.{table_name};`
- For spatial: apply per-table geometry conversion (same SQL patterns as recreate_spatial_ducklake.sql):
  - Standard 6 tables: `SELECT * EXCLUDE({geom_col}), ST_GeomFromWKB({geom_col}) AS {geom_col} FROM source.{table_name}`
  - ca_boundaries_bgc_tbl: `SELECT * EXCLUDE(geom), ST_Multi(geom) AS geom FROM source.ca_boundaries_bgc_tbl`
  - lsoa_2021_lep_tbl: `SELECT * EXCLUDE(shape), ST_GeomFromWKB(shape) AS shape, ST_IsValid(ST_GeomFromWKB(shape)) AS geom_valid FROM source.lsoa_2021_lep_tbl`
- Write SQL to temp file, execute via `duckdb -init "{tmp}" -c "SELECT 1;" -no-stdin`, clean up temp file
- IMPORTANT: Build ALL 18 DROP+CREATE statements into a single SQL file and execute once (one DuckDB CLI call for all DuckLake operations), not per-table. This avoids 18 separate extension installs and S3 credential setups.

**(B) Pins export** (via R):
- For non-spatial standard tables (<5M rows): `dbReadTable()` then `pin_write()` with type="parquet"
- For large non-spatial tables (>=5M rows, i.e. raw_domestic_epc_certificates_tbl): chunked `COPY TO` with LIMIT/OFFSET + `pin_upload()` (same pattern as export_pins.R)
- For spatial tables: `COPY TO` GeoParquet via DuckDB CLI (using source DB + spatial extension, same pattern as export_spatial_pins.R), then `pin_upload()` with spatial metadata (geometry_column, geometry_type, crs)
- All pins include custom metadata: source_db, columns (descriptions), column_types

**Row count validation:**
- After each table's DuckLake export, query `SELECT COUNT(*) FROM lake.{table_name}` via DuckDB CLI
- Compare with source count (already obtained from source DB)
- Record pass/fail per table
- Note: lsoa_2021_lep_tbl has geom_valid extra column so column count differs, but row count should match

**Results tracking:**
- data.frame with columns: table_name, is_spatial, row_count, ducklake_ok, pin_ok, row_match, time_secs, error
- tryCatch around each table's processing (continue on error, accumulate failures)

**Console summary:**
- At end of run, print formatted table: table name (left-aligned, 35 chars), rows (right-aligned, 10 chars), time in seconds (right-aligned, 8 chars), status (PASS/FAIL, right-aligned, 6 chars)
- Print totals: tables refreshed N/18, total rows, total time, failures count
- If any failures, print failure details

**Important implementation notes:**
- DuckLake operations MUST use DuckDB CLI (R duckdb package v1.4.4 lacks ducklake extension)
- Data only — do NOT re-apply column comments or recreate views (per user decision)
- Use forward slashes in paths passed to DuckDB CLI (`gsub("\\\\", "/", path)`)
- Use `writeLines(..., useBytes = TRUE)` for SQL temp files
- The refresh script does NOT delete the .ducklake catalogue file — it drops and recreates tables within the existing catalogue
  </action>
  <verify>
    <automated>Rscript -e "source('scripts/refresh.R')" 2>&1 | tail -30</automated>
    <manual>Check console output shows all 18 tables with PASS status and matching row counts</manual>
    <sampling_rate>run after this task commits</sampling_rate>
  </verify>
  <done>
- `scripts/refresh.R` exists and runs without error
- All 18 tables re-exported to DuckLake (DROP + CREATE TABLE)
- All 18 tables re-exported as pins to S3 (parquet or GeoParquet)
- Row counts match between source and DuckLake for all 18 tables
- Console summary shows table name, row count, time, pass/fail for each table
- Spatial edge cases handled (ST_Multi for ca_boundaries_bgc_tbl, geom_valid for lsoa_2021_lep_tbl)
- Large table (EPC) uses chunked pin_upload pattern
  </done>
</task>

</tasks>

<verification>
1. `Rscript scripts/refresh.R` completes without error
2. Console summary shows 18/18 tables with PASS status
3. DuckLake tables queryable: `duckdb -c "INSTALL ducklake; LOAD ducklake; INSTALL httpfs; LOAD httpfs; INSTALL aws; LOAD aws; CREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain); ATTACH 'ducklake:data/mca_env.ducklake' AS lake (DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/', READ_ONLY); SELECT table_name FROM information_schema.tables WHERE table_catalog = 'lake' ORDER BY table_name;" -no-stdin` returns 18 tables
4. Pins list includes all 18 tables: `Rscript -e "library(pins); b <- board_s3('stevecrawshaw-bucket', prefix='pins/', region='eu-west-2', versioned=TRUE); cat(length(pin_list(b)))"`
</verification>

<success_criteria>
- Single command `Rscript scripts/refresh.R` refreshes all 18 tables end-to-end
- DuckLake history preserved (previous snapshots accessible via time travel)
- Pins versions updated (analysts get latest data by default)
- Row count validation confirms data integrity for every table
</success_criteria>

<output>
After completion, create `.planning/phases/05-refresh-pipeline-and-data-catalogue/05-01-SUMMARY.md`
</output>
