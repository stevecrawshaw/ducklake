---
phase: 03-ducklake-catalogue
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/create_ducklake.sql
  - scripts/create_ducklake.R
autonomous: true

must_haves:
  truths:
    - "DuckLake catalogue exists on S3 at ducklake:s3://stevecrawshaw-bucket/ducklake/mca_env.ducklake"
    - "All 18 tables are registered in the catalogue with data stored as parquet under ducklake/data/"
    - "An analyst can ATTACH the catalogue and run SELECT on any table"
  artefacts:
    - path: "scripts/create_ducklake.sql"
      provides: "SQL script for catalogue creation and table registration"
      contains: "COPY FROM DATABASE"
    - path: "scripts/create_ducklake.R"
      provides: "R wrapper script that executes the SQL against DuckDB"
      contains: "dbExecute"
  key_links:
    - from: "scripts/create_ducklake.R"
      to: "data/mca_env_base.duckdb"
      via: "DuckDB connection reading source tables"
      pattern: "mca_env_base.duckdb"
    - from: "scripts/create_ducklake.sql"
      to: "s3://stevecrawshaw-bucket/ducklake/"
      via: "ATTACH ducklake: with DATA_PATH"
      pattern: "stevecrawshaw-bucket/ducklake"
---

<objective>
Create a DuckLake catalogue on S3 and register all 18 tables (10 non-spatial + 8 spatial) from the source DuckDB.

Purpose: This is the foundation for Phase 3 -- without a working catalogue, no subsequent comments, views, or time travel can be added.
Output: A working DuckLake catalogue on S3 that analysts can ATTACH and query.
</objective>

<execution_context>
@C:\Users\steve.crawshaw\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\steve.crawshaw\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ducklake-catalogue/03-RESEARCH.md
@.planning/phases/03-ducklake-catalogue/03-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DuckLake catalogue and register all 18 tables</name>
  <files>scripts/create_ducklake.sql, scripts/create_ducklake.R</files>
  <action>
Create two files:

**scripts/create_ducklake.sql** -- The SQL script that:
1. Installs and loads extensions: `ducklake`, `httpfs`, `aws`
2. Creates an S3 secret using `credential_chain` (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain) -- do NOT hardcode keys
3. Creates a placeholder object in `s3://stevecrawshaw-bucket/ducklake/data/` if needed (research pitfall 3: DATA_PATH must already exist)
4. Creates the DuckLake catalogue: `ATTACH 'ducklake:s3://stevecrawshaw-bucket/ducklake/mca_env.ducklake' AS lake (DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/')`
5. Attaches the source database: `ATTACH 'data/mca_env_base.duckdb' AS source (READ_ONLY)`
6. Copies all tables: `COPY FROM DATABASE source TO lake`
   - If COPY FROM DATABASE fails on spatial types (WKB_BLOB/GEOMETRY), fall back to copying the 10 non-spatial tables individually with `CREATE TABLE lake.{name} AS SELECT * FROM source.{name}`, then copy the 8 spatial tables individually, catching errors and logging any failures.
   - Do NOT copy views -- views will be created manually in plan 03-02 because 3 of 7 views depend on spatial functions.

**scripts/create_ducklake.R** -- An R script that:
1. Connects to DuckDB (in-memory instance)
2. Reads and executes the SQL script statement by statement (split on `;`)
3. Uses tryCatch for each statement so failures are logged but don't halt execution
4. Prints progress for each step
5. After table registration, verifies by querying `SHOW TABLES` on the lake database and printing the result
6. Verifies a sample query: `SELECT COUNT(*) FROM lake.ca_la_lookup_tbl` (should return 216 rows)

Important considerations:
- The source DuckDB file is at `data/mca_env_base.duckdb` (relative to project root)
- Use `credential_chain` provider, not hardcoded AWS keys
- COPY FROM DATABASE may try to copy views too -- if that causes errors from spatial function references (st_transform, geopoint_from_blob), that is expected; the views will be recreated manually in 03-02
- If the .ducklake file on S3 does NOT work (open question 1 from research), log the error clearly and document the finding. The fallback would be a local .ducklake file, but try S3 first.
  </action>
  <verify>
Run `scripts/create_ducklake.R` from the project root. It should:
1. Complete without fatal errors (view copy errors are acceptable)
2. Print all 18 table names from SHOW TABLES
3. Print row count for ca_la_lookup_tbl (216)

Then verify from a fresh DuckDB CLI session:
```sql
INSTALL ducklake; LOAD ducklake;
CREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain);
ATTACH 'ducklake:s3://stevecrawshaw-bucket/ducklake/mca_env.ducklake' AS lake (READ_ONLY);
SELECT table_name FROM information_schema.tables WHERE table_schema = 'main' AND table_catalog = 'lake';
SELECT COUNT(*) FROM lake.boundary_lookup_tbl;
```
  </verify>
  <done>
All 18 tables are registered in the DuckLake catalogue on S3. An analyst can ATTACH the catalogue and run SELECT queries on any table. The catalogue file lives at s3://stevecrawshaw-bucket/ducklake/mca_env.ducklake with data under s3://stevecrawshaw-bucket/ducklake/data/.
  </done>
</task>

</tasks>

<verification>
1. DuckLake catalogue file exists at s3://stevecrawshaw-bucket/ducklake/mca_env.ducklake
2. Data directory exists at s3://stevecrawshaw-bucket/ducklake/data/ with parquet files
3. All 18 tables visible via SHOW TABLES on attached catalogue
4. SELECT queries return correct data (spot check 2-3 tables)
5. Catalogue is accessible via READ_ONLY attach from a fresh DuckDB session
</verification>

<success_criteria>
An analyst can run `ATTACH 'ducklake:s3://stevecrawshaw-bucket/ducklake/mca_env.ducklake' AS lake (READ_ONLY)` from a fresh DuckDB session and query any of the 18 tables.
</success_criteria>

<output>
After completion, create `.planning/phases/03-ducklake-catalogue/03-01-SUMMARY.md`
</output>
