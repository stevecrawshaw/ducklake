# Phase 6: Analyst Documentation - Research

**Researched:** 2026-02-25
**Domain:** Technical documentation (Quarto, R, Python, DuckLake, pins)
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions
- Primary audience: internal WECA staff, mainly R/tidyverse analysts
- Assume nothing: cover R/Python package installation, DuckDB installation, and basic SQL orientation
- Include AWS credential setup (how to request access, configure locally, verify connectivity)
- Can reference internal systems, Slack channels, and team contacts
- Single combined guide covering everything: setup, pins, DuckLake, spatial
- Quarto document (.qmd) rendering to HTML and PDF
- PDF output via branded typst template (https://github.com/stevecrawshaw/typst-template)
- HTML output with WECA corporate branding (use /weca-branding skill)
- R is the primary language throughout the guide
- Python equivalents provided in an appendix section
- Include both: a quick summary table of all 18 datasets in the doc + instructions on querying the DuckLake catalogue programmatically for full details
- All code examples executable Quarto chunks with visible output
- Cover: connection, reading tables, filtering, joining, aggregation, spatial operations
- Dedicated spatial section: reading GeoParquet, converting to sf/geopandas, plotting a quick map
- Time travel: brief mention with syntax example, not a full section
- CRS note for spatial data: analysts must set CRS explicitly (DuckDB GeoParquet doesn't embed it)
- Docs live in the GitHub repo (primary distribution)
- Include support contact info (Slack channel or named person for data platform questions)

### Claude's Discretion
- Document flow structure (linear tutorial vs quick-start + reference -- decide based on what works best for the content)
- Troubleshooting approach (dedicated section vs inline tips -- decide based on likely pain points)
- Exact section ordering and headings
- Level of SQL primer coverage (enough to get started, not a SQL course)

### Deferred Ideas (OUT OF SCOPE)
None -- discussion stayed within phase scope
</user_constraints>

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| INFRA-03 | Documentation for analysts on how to access data via pins (R and Python) | Existing `test_interop.R`, `test_interop.py`, `validate_pins_r.R`, `validate_pins.py` provide verified working code patterns for board creation, pin_list, pin_read, pin_meta, spatial pin handling |
| INFRA-04 | Documentation for analysts on how to attach and query DuckLake | Existing `validate_ducklake.R`, `verify_ducklake.sql` provide verified DuckLake attach syntax, SQL patterns, time travel, data change feed |
</phase_requirements>

## Summary

Phase 6 produces a single Quarto document (`docs/analyst-guide.qmd`) that renders to both HTML (with WECA branding) and PDF (via branded typst template). The document is a comprehensive analyst onboarding guide covering AWS credential setup, R/Python package installation, pins access, DuckLake catalogue queries, and spatial data handling.

The codebase already contains all the verified working patterns needed. The existing scripts (`test_interop.R`, `validate_pins_r.R`, `validate_ducklake.R`, `spike_spatial.R`, `verify_ducklake.sql`) provide battle-tested code examples that can be distilled into documentation. The data catalogue tables (`datasets_catalogue`, `columns_catalogue`) generated by Phase 5's refresh pipeline provide the dataset summary content.

**Primary recommendation:** Create a single `.qmd` file with a linear tutorial flow (setup -> pins -> DuckLake -> spatial -> appendix), using executable R code chunks and a dedicated Python appendix. Install the typst template extension from the user's GitHub repo and configure dual HTML/PDF output in the YAML front matter.

## Standard Stack

### Core
| Tool | Version | Purpose | Why Standard |
|------|---------|---------|--------------|
| Quarto | >= 1.4 | Document authoring and rendering | Industry standard for reproducible technical docs; supports R/Python code chunks, multiple output formats, Typst PDF |
| Typst | (bundled with Quarto >= 1.4) | PDF rendering engine | Faster than LaTeX, native Quarto support since 1.4, WECA template already exists |
| weca-report extension | from stevecrawshaw/typst-template | Branded PDF output | User's own extension with WECA branding, fonts, assets |

### Supporting
| Tool | Version | Purpose | When to Use |
|------|---------|---------|-------------|
| R pins | >= 1.3.0 | S3 board access examples | All R pins code examples in doc |
| R arrow | >= 14.0 | Parquet reading (large/multi-file pins) | Large table and spatial data examples |
| R sf | >= 1.0 | Spatial data handling | Spatial section examples |
| R DBI + duckdb | >= 1.4.4 | DuckDB connection from R | DuckLake query examples |
| Python pins | >= 0.9.1 | S3 board access (Python appendix) | Python appendix |
| Python geopandas | >= 1.0 | Spatial data (Python appendix) | Python appendix spatial section |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Quarto + Typst | Quarto + LaTeX | LaTeX is slower, heavier dependency; Typst is the modern choice and user already has a template |
| Single .qmd file | Multi-page Quarto book | Overkill for one guide; single file is simpler to maintain and distribute |
| Executable chunks | Static code blocks | Executable chunks prove the code works; static blocks can go stale |

### Installation

Quarto is **not currently installed** on this machine. Before the plan can execute:

```bash
# Install Quarto (Windows)
# Download from https://quarto.org/docs/download/
# Or via winget:
winget install Posit.Quarto

# Install the WECA typst template extension into the project
cd C:/Users/steve.crawshaw/projects/ducklake
quarto add stevecrawshaw/typst-template
```

R packages (likely already installed given existing scripts):
```r
install.packages(c("quarto", "pins", "arrow", "sf", "duckdb", "DBI"))
```

## Architecture Patterns

### Recommended Document Structure

The document should follow a linear tutorial flow. This is the right choice because: (a) the primary audience are analysts going from zero, (b) each section builds on the previous (credentials -> connection -> queries), and (c) a linear flow maps naturally to the 10-minute success criterion.

```
docs/
├── analyst-guide.qmd          # Single combined guide
├── _brand.yml                  # WECA brand config for HTML (optional)
└── custom.scss                 # WECA-branded CSS for HTML output
```

### Pattern 1: Quarto YAML Front Matter (Dual Output)

**What:** Configure both HTML and PDF output from a single .qmd file.
**When to use:** Always -- this is the document configuration.

```yaml
---
title: "WECA Data Platform: Analyst Guide"
subtitle: "Accessing shared datasets via pins and DuckLake"
author: "West of England Combined Authority"
date: last-modified
format:
  html:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 3
    code-fold: false
    embed-resources: true
  weca-report-typst:
    toc: true
    toc-depth: 3
    section-numbering: "1.1"
execute:
  echo: true
  eval: false
  warning: false
---
```

**Key decisions in this YAML:**
- `eval: false` globally -- code examples are shown but not executed during render (the guide documents how to run code, but rendering shouldn't require live AWS credentials or S3 access)
- `echo: true` -- all code is visible
- `embed-resources: true` for HTML -- produces a single self-contained HTML file for easy distribution
- The `weca-report-typst` format comes from the installed extension

### Pattern 2: Executable Code Chunks with Visible Output

**What:** Show R code with representative output without requiring live execution.
**When to use:** For all code examples in the guide.

The user wants "executable Quarto chunks with visible output". Since rendering requires AWS credentials and live S3 access, the recommended approach is:

**Option A (recommended):** Use `eval: false` globally but include representative output as text blocks or screenshots beneath each code chunk. This means the .qmd can be rendered by anyone without credentials.

**Option B:** Use `eval: true` (requires credentials at render time). Risk: rendering breaks if credentials expire or network is unavailable.

Recommendation: **Option A** -- `eval: false` with hand-crafted representative output. The user decision says "executable chunks with visible output", but the practical constraint is that rendering happens on CI or by anyone cloning the repo. Use `#| eval: false` and show expected output in a separate ```` ```text ```` block.

### Pattern 3: WECA HTML Branding via Custom SCSS

**What:** Apply WECA brand colours, typography, and logo to HTML output.
**When to use:** For the HTML render.

```scss
// custom.scss
/*-- scss:defaults --*/
$font-family-sans-serif: "Trebuchet MS", "Open Sans", sans-serif;
$link-color: #40A832;
$body-color: #1F1F1F;

/*-- scss:rules --*/
.quarto-title-banner {
  background-color: #1D4F2B;
  color: #FFFFFF;
}

h1, h2, h3 {
  color: #1D4F2B;
  font-family: "Trebuchet MS", sans-serif;
}

.callout-tip {
  border-left-color: #40A832;
}
```

### Pattern 4: Section Structure (Linear Tutorial)

**What:** Ordered sections that build on each other.

```
1. Introduction (what this guide covers, 10-minute promise)
2. Prerequisites & Setup
   2.1. R packages
   2.2. DuckDB installation
   2.3. AWS credentials (incorporate existing analyst-aws-setup.md content)
3. Quick Reference: Available Datasets (summary table of all 18 + views)
4. Accessing Data via Pins (R)
   4.1. Creating a board
   4.2. Listing available datasets
   4.3. Reading a dataset
   4.4. Accessing metadata (descriptions, column info)
   4.5. Large datasets (arrow fallback for EPC table)
5. Querying the DuckLake Catalogue
   5.1. Installing extensions
   5.2. Attaching the catalogue
   5.3. Basic SQL queries
   5.4. Using views (WECA-filtered)
   5.5. Querying the data catalogue programmatically
   5.6. Time travel (brief syntax example)
6. Working with Spatial Data
   6.1. Reading GeoParquet pins
   6.2. Converting to sf objects
   6.3. Setting CRS (mandatory -- DuckDB doesn't embed it)
   6.4. Plotting a quick map
7. Troubleshooting (dedicated section -- see reasoning below)
8. Support & Contact
Appendix A: Python Equivalents
   A.1. Pins access
   A.2. DuckLake queries
   A.3. Spatial data with geopandas
Appendix B: SQL Quick Reference
```

**Troubleshooting as dedicated section (discretion decision):** A dedicated troubleshooting section is better than inline tips because: (a) the same issues (credentials, region, CRS) recur across sections, (b) analysts will search for "troubleshooting" when stuck, (c) inline tips interrupt the tutorial flow.

**SQL primer as appendix (discretion decision):** A brief SQL appendix (SELECT, WHERE, JOIN, GROUP BY with DuckLake examples) serves analysts who know R but not SQL, without slowing down SQL-literate readers.

### Anti-Patterns to Avoid
- **Separate documents per topic:** User explicitly wants a single combined guide. Don't split into multiple files.
- **eval: true without credential guards:** Code that requires AWS access will break rendering for anyone without credentials.
- **Referencing file paths only the author has:** The .ducklake catalogue file is local. Documentation must explain how analysts get this file.
- **Assuming sfarrow works:** It doesn't with DuckDB GeoParquet. Always document the arrow::read_parquet + sf::st_as_sf pattern.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| PDF branding | Custom LaTeX template | stevecrawshaw/typst-template (weca-report extension) | Already built, tested, has WECA fonts/assets |
| HTML branding | Inline CSS | Custom SCSS with WECA colour variables from /weca-branding skill | Maintainable, theme-consistent |
| Dataset inventory | Manually typed table | Query datasets_catalogue from DuckLake or pin | Auto-generated by refresh pipeline, always current |
| Code examples | New untested snippets | Distilled patterns from existing validated scripts | test_interop.R, validate_pins_r.R, validate_ducklake.R are battle-tested |

**Key insight:** Nearly all code examples already exist in the `scripts/` directory as validated, working code. The documentation task is distillation and presentation, not invention.

## Common Pitfalls

### Pitfall 1: DuckLake Catalogue File Distribution
**What goes wrong:** The `.ducklake` catalogue file (`data/mca_env.ducklake`) is local. Analysts cannot attach DuckLake without this file.
**Why it happens:** DuckDB cannot create .ducklake files on S3 (Phase 3 finding).
**How to avoid:** Document clearly that analysts must clone the repo (or download the .ducklake file) to use DuckLake. The file is small metadata only -- data lives on S3.
**Warning signs:** Analyst gets "file not found" when trying to ATTACH.

### Pitfall 2: CRS Not Embedded in GeoParquet
**What goes wrong:** Analysts load spatial data and plot it, but coordinates look wrong or overlay fails.
**Why it happens:** DuckDB COPY TO does not write CRS metadata into GeoParquet files. geopandas reports CRS as OGC:CRS84 (incorrect). sf reports NA.
**How to avoid:** Document explicitly: "After reading GeoParquet, you MUST set the CRS". Include the CRS for each spatial table (most are EPSG:27700, one is EPSG:4326). Reference pin metadata which stores the correct CRS.
**Warning signs:** Map looks distorted, CRS shows as NA or OGC:CRS84.

### Pitfall 3: sfarrow Fails on DuckDB GeoParquet
**What goes wrong:** Analyst tries `sfarrow::st_read_parquet()` and gets an error about missing CRS metadata.
**Why it happens:** sfarrow requires full GeoParquet metadata including CRS; DuckDB's GeoParquet output lacks this.
**How to avoid:** Document the correct pattern: `arrow::read_parquet()` then `sf::st_as_sf()`. Never mention sfarrow as a viable approach.
**Warning signs:** Error about missing "geo" key or CRS in parquet metadata.

### Pitfall 4: Python pins board_s3 Path Format
**What goes wrong:** Python analyst creates board with wrong path and gets connection errors.
**Why it happens:** Python `board_s3()` uses `"bucket/prefix"` format (e.g., `"stevecrawshaw-bucket/pins"`), not separate bucket and prefix parameters like the R version.
**How to avoid:** Show the exact Python board creation syntax prominently. Contrast with R syntax.
**Warning signs:** S3 permission errors or "bucket not found".

### Pitfall 5: Python pin_read Fails on Multi-File Pins
**What goes wrong:** Python `board.pin_read("raw_domestic_epc_certificates_tbl")` throws an error.
**Why it happens:** The EPC table is exported as multiple parquet files (chunked pin_upload). Python pins library cannot handle multi-file pins.
**How to avoid:** Document the arrow/duckdb fallback pattern for large multi-file pins. Show how to detect multi-file pins.
**Warning signs:** Error on pin_read for the EPC table specifically.

### Pitfall 6: Quarto Not Installed
**What goes wrong:** Build fails with "quarto: command not found".
**Why it happens:** Quarto is not currently installed on the development machine.
**How to avoid:** Plan must include a prerequisite task to install Quarto and the typst template extension before authoring begins.
**Warning signs:** Any quarto command fails.

### Pitfall 7: R duckdb Package Cannot Use DuckLake
**What goes wrong:** Analyst tries `DBI::dbConnect(duckdb::duckdb()) |> dbExecute("INSTALL ducklake")` from R and it fails or the extension isn't available.
**Why it happens:** R duckdb package (v1.4.4) lacks the ducklake extension. All DuckLake operations in the project use DuckDB CLI.
**How to avoid:** Document two distinct access paths: (a) DuckDB CLI for DuckLake SQL queries, (b) R pins for data frame access. Be explicit that DuckLake queries require the CLI, not the R package.
**Warning signs:** Extension installation error in R.

## Code Examples

Verified patterns from existing project scripts:

### R: Create S3 Board and Read a Pin
```r
# Source: scripts/test_interop.R (verified working)
library(pins)

board <- board_s3(
  bucket = "stevecrawshaw-bucket",
  prefix = "pins/",
  region = "eu-west-2",
  versioned = TRUE
)

# List all datasets
pin_list(board)

# Read a dataset
df <- pin_read(board, "ca_la_lookup_tbl")

# Read metadata (column descriptions)
meta <- pin_meta(board, "ca_la_lookup_tbl")
meta$title
meta$user$columns  # column descriptions
```

### R: Read Spatial Pin as sf Object
```r
# Source: scripts/spike_spatial.R (verified working)
library(pins)
library(arrow)
library(sf)

board <- board_s3(
  bucket = "stevecrawshaw-bucket",
  prefix = "pins/",
  region = "eu-west-2",
  versioned = TRUE
)

# Download GeoParquet
pin_path <- pin_download(board, "bdline_ua_lep_tbl")

# Read with arrow (NOT sfarrow -- it fails on DuckDB GeoParquet)
arrow_tbl <- arrow::read_parquet(pin_path, as_data_frame = FALSE)
sf_obj <- sf::st_as_sf(as.data.frame(arrow_tbl))

# CRS is NA -- set it explicitly (EPSG:27700 for most spatial tables)
sf_obj <- sf::st_set_crs(sf_obj, 27700)

# Quick plot
plot(sf_obj["shape"])
```

### DuckDB CLI: Attach DuckLake and Query
```sql
-- Source: scripts/verify_ducklake.sql (verified working)
INSTALL ducklake; LOAD ducklake;
CREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain);
ATTACH 'ducklake:data/mca_env.ducklake' AS lake
  (READ_ONLY, DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/');

-- List tables
SHOW TABLES;

-- Query a WECA view
SELECT DISTINCT local_authority_code
FROM lake.la_ghg_emissions_weca_vw;

-- Browse the data catalogue
SELECT name, description, type, row_count
FROM lake.datasets_catalogue
ORDER BY type, name;

-- Time travel (query at a previous snapshot)
SELECT COUNT(*) FROM lake.ca_la_lookup_tbl AT (VERSION => 1);
```

### Python: Create Board and Read Pin
```python
# Source: scripts/test_interop.py (verified working)
import os
os.environ.setdefault("AWS_DEFAULT_REGION", "eu-west-2")

from pins import board_s3

# Note: Python uses "bucket/prefix" format (no trailing slash)
board = board_s3("stevecrawshaw-bucket/pins", versioned=True)

# List datasets
board.pin_list()

# Read a dataset
df = board.pin_read("ca_la_lookup_tbl")

# Read metadata
meta = board.pin_meta("ca_la_lookup_tbl")
meta.title
meta.user["columns"]  # column descriptions
```

### Python: Read Spatial Pin with geopandas
```python
# Source: scripts/spike_spatial.R Python validation block (verified working)
import os
os.environ.setdefault("AWS_DEFAULT_REGION", "eu-west-2")

from pins import board_s3
import geopandas as gpd

board = board_s3("stevecrawshaw-bucket/pins", versioned=True)
path = board.pin_download("bdline_ua_lep_tbl")
gdf = gpd.read_parquet(path[0])

# CRS may show as OGC:CRS84 -- override to correct CRS
gdf = gdf.set_crs("EPSG:27700", allow_override=True)

# Quick plot
gdf.plot()
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| LaTeX for PDF | Typst via Quarto >= 1.4 | Jan 2024 | Faster rendering, simpler templates, native Quarto support |
| R Markdown | Quarto | 2022 | Multi-language, better ecosystem, actively developed |
| sfarrow for GeoParquet | arrow + sf | Project finding (Phase 4) | sfarrow fails on DuckDB GeoParquet without CRS |

**Deprecated/outdated:**
- sfarrow: Incompatible with DuckDB GeoParquet output (missing CRS in geo metadata)
- R duckdb package for DuckLake: v1.4.4 lacks ducklake extension; use DuckDB CLI instead

## Open Questions

1. **Quarto Installation**
   - What we know: Quarto is not installed on the development machine. The typst template extension exists at stevecrawshaw/typst-template.
   - What's unclear: Whether `winget install Posit.Quarto` will work in the environment, or manual download is needed.
   - Recommendation: Plan Wave 0 should include Quarto installation and extension setup as a prerequisite task. If installation fails, fall back to plain Markdown (loses executable chunks and dual output).

2. **eval: true vs eval: false for Code Chunks**
   - What we know: User wants "executable chunks with visible output". Live execution requires AWS credentials at render time.
   - What's unclear: Whether the rendering environment will always have credentials available.
   - Recommendation: Use `eval: false` with representative output shown as text blocks. This ensures the guide can be rendered by anyone. If the user insists on live execution, use `eval: true` with a note that rendering requires credentials.

3. **DuckLake Catalogue File Distribution**
   - What we know: `data/mca_env.ducklake` is local (cannot be created on S3). Analysts need this file.
   - What's unclear: Exact distribution mechanism (Git repo clone? Shared drive? Direct download?).
   - Recommendation: Document "clone the repo" as the primary path. The .ducklake file is already in the repo's `data/` directory.

4. **Contact Information for Support Section**
   - What we know: User wants support contact info (Slack channel or named person).
   - What's unclear: Exact Slack channel name or contact person.
   - Recommendation: Use a placeholder (`[#data-platform]` or `[Data Platform Team]`) that the user fills in before publishing.

## Existing Assets to Incorporate

The following existing files contain content that should be incorporated (not duplicated) into the guide:

| Existing File | Content | How to Use |
|---------------|---------|------------|
| `docs/analyst-aws-setup.md` | Complete AWS credential setup guide | Incorporate into Prerequisites section (already written, validated) |
| `scripts/verify_ducklake.sql` | DuckLake attach + query examples | Distil into DuckLake section |
| `scripts/test_interop.R` | R pins board creation + pin read/write | Distil into Pins section |
| `scripts/test_interop.py` | Python pins board creation + pin read | Distil into Python appendix |
| `scripts/validate_pins_r.R` | R pin validation patterns (large tables, metadata) | Reference for edge cases |
| `scripts/validate_pins.py` | Python pin validation (multi-file fallback) | Reference for Python edge cases |
| `scripts/spike_spatial.R` | Spatial pipeline (sf, geopandas, CRS handling) | Distil into Spatial section |
| `scripts/validate_ducklake.R` | DuckLake CLI patterns, time travel, views | Reference for DuckLake patterns |

## WECA Branding Details

### HTML Output
From the /weca-branding skill:
- **Primary colour:** West Green #40A832
- **Dark colour (headings):** Forest Green #1D4F2B
- **Typography:** Trebuchet MS (Bold for headings, Regular for body)
- **Logo:** `C:\Users\steve.crawshaw\.claude\skills\weca-branding\weca_logo.jpg` (or .webp for web)
- Apply via custom SCSS theme file in Quarto

### PDF Output
From the typst template repo (`stevecrawshaw/typst-template`):
- Extension name: `weca-report`
- Format key in YAML: `weca-report-typst`
- Template includes: `_extensions/weca-report/` with `_extension.yml`, `typst-template.typ`, `typst-show.typ`, `assets/`, `fonts/`
- Install via: `quarto add stevecrawshaw/typst-template`

## Sources

### Primary (HIGH confidence)
- `/websites/quarto` (Context7) - Typst format configuration, code execution options, multi-format output, custom SCSS theming
- Existing project scripts (all in `scripts/` directory) - Verified working code patterns for pins, DuckLake, spatial data
- `docs/analyst-aws-setup.md` - Existing credential setup documentation
- WECA branding skill (`~/.claude/skills/weca-branding/SKILL.md`) - Complete brand colour palette, typography, logo specifications

### Secondary (MEDIUM confidence)
- GitHub `stevecrawshaw/typst-template` (WebFetch) - Template structure confirmed: `_extensions/weca-report/` with standard Quarto extension layout, `template.qmd` uses `weca-report-typst` format key
- Phase 5 summary (`05-02-SUMMARY.md`) - Catalogue table structure: datasets_catalogue (30 rows), columns_catalogue (411 rows)
- Project STATE.md - All accumulated decisions and known issues from Phases 1-5

### Tertiary (LOW confidence)
- Quarto installation on Windows (`winget install Posit.Quarto`) - not verified on this machine; quarto is not currently on PATH

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Quarto, R, Python are well understood; typst template verified via GitHub
- Architecture: HIGH - document structure follows standard Quarto patterns; all code examples from verified scripts
- Pitfalls: HIGH - all pitfalls documented from actual project findings (Phases 2-5)

**Research date:** 2026-02-25
**Valid until:** 2026-03-25 (stable domain, no fast-moving dependencies)
