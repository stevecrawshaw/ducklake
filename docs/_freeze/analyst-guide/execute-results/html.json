{
  "hash": "a3b197e5c24475c695b63124ff1a961e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"WECA Data Platform: Analyst Guide\"\nsubtitle: \"Accessing shared datasets via pins and DuckLake\"\nauthor: \"West of England Combined Authority\"\ndate: last-modified\ntitle-block-banner: true\nformat:\n  html:\n    code-fold: false\n  weca-report-typst:\n    toc: true\n    toc-depth: 3\nexecute:\n  echo: true\n  eval: true\n  warning: false\n---\n\n![](weca_logo.jpg){width=200px fig-align=\"left\"}\n\n# Introduction\n\nWECA's shared data platform provides **18 curated datasets** covering local authority lookups, greenhouse gas emissions, energy performance certificates, deprivation indices, postcode centroids, tenure, and spatial boundaries. All data is stored on Amazon S3 and accessible through two complementary routes:\n\n- **Pins (R or Python):** Read datasets directly into data frames. Best for quick exploratory analysis, filtering, and visualisation.\n- **DuckLake (SQL via DuckDB CLI):** Query datasets with SQL, join across tables, use pre-built WECA-filtered views, and browse the data catalogue. Best for complex queries and when you need to work across multiple tables.\n\nBoth routes read from the same underlying data. Choose whichever fits your workflow -- or use both.\n\n::: {.callout-tip}\n## The 10-minute promise\nIf you follow the Prerequisites section below, you will be able to read your first dataset within 10 minutes.\n:::\n\nThis guide covers everything you need: package installation, AWS credential setup, reading data via pins, querying the DuckLake catalogue with SQL, working with spatial data, and troubleshooting common issues. Python equivalents are provided in [Appendix A](#appendix-a-python-equivalents).\n\n\n::: {.cell}\n\n:::\n\n\n# Prerequisites and Setup\n\nBefore accessing data, you need three things: R packages, the DuckDB command-line tool (for SQL access), and AWS credentials. This section walks through each.\n\n## R Packages\n\nInstall the core packages. This guide assumes you already have the tidyverse installed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\"pins\", \"arrow\", \"sf\", \"duckdb\", \"DBI\"))\n```\n:::\n\n\n| Package | Purpose |\n|---------|---------|\n| `pins` | Connect to the S3 board, list/read/download datasets |\n| `arrow` | Read parquet files (required for large and spatial datasets) |\n| `sf` | Handle spatial data (GeoParquet to sf objects) |\n| `duckdb` | DuckDB database driver for R (used for local queries) |\n| `DBI` | Database interface (works with duckdb) |\n\n: {tbl-colwidths=\"[20,80]\"}\n\n## DuckDB Installation\n\nThe DuckDB command-line interface (CLI) is required for querying the DuckLake catalogue. The R `duckdb` package (v1.4.4) does not include the `ducklake` extension, so SQL queries against the catalogue must be run through the CLI.\n\n1. Download the DuckDB CLI from <https://duckdb.org/docs/installation/>.\n2. Place the `duckdb` executable somewhere on your PATH.\n3. Verify installation:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nduckdb --version\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1.4.1 (Andium) b390a7c376\n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\nThe DuckDB CLI is only needed for DuckLake SQL queries ([Section 5](#querying-the-ducklake-catalogue)). For reading data via pins, you only need the R packages above.\n:::\n\n## AWS Credentials\n\nYou need AWS credentials to access the shared S3 bucket (`stevecrawshaw-bucket`). Request an access key ID and secret access key from the data owner (distributed via Keeper).\n\n### Step 1: Create the credentials file\n\nCreate the file at:\n\n- **Windows:** `C:\\Users\\USERNAME\\.aws\\credentials`\n- **Linux/Mac:** `~/.aws/credentials`\n\nIf the `.aws` directory does not exist, create it first. Paste the following content, replacing the placeholders with your actual keys - get these from Steve:\n\n```ini\n[default]\naws_access_key_id = <YOUR_ACCESS_KEY_ID>\naws_secret_access_key = <YOUR_SECRET_ACCESS_KEY>\n```\n\n### Step 2: Create the config file\n\nCreate the file at:\n\n- **Windows:** `C:\\Users\\USERNAME\\.aws\\config`\n- **Linux/Mac:** `~/.aws/config`\n\nPaste the following content:\n\n```ini\n[default]\nregion = eu-west-2\noutput = json\n```\n\n::: {.callout-warning}\n## Region must be eu-west-2\nThe S3 bucket is in the `eu-west-2` (London) region. Using any other region will cause \"Access Denied\" errors or 301 redirects. Always set `region = eu-west-2`.\n:::\n\n### Step 3: Verify access\n\nPick whichever tool you use day-to-day. A successful result from any one method confirms your credentials are working.\n\n**R (pins):**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pins)\nboard <- board_s3(bucket = \"stevecrawshaw-bucket\",\n                  prefix = \"pins/\",\n                   region = \"eu-west-2\")\npin_list(board)\n```\n:::\n\n\n**DuckDB CLI:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nINSTALL aws; LOAD aws;\nCREATE OR REPLACE SECRET (\n    TYPE s3, PROVIDER credential_chain,\n    CHAIN config, REGION 'eu-west-2'\n);\nSELECT * FROM glob('s3://stevecrawshaw-bucket/*');\n```\n:::\n\n\n**AWS CLI (optional):**\n\n\n::: {.cell}\n\n```{.bash .cell-code}\naws s3 ls s3://stevecrawshaw-bucket/\n```\n:::\n\n\n::: {.callout-tip}\n## Troubleshooting credentials\nIf you get \"Access Denied\" or \"No credentials\" errors, see the [Troubleshooting](#troubleshooting) section. Common causes: wrong region, missing profile name `[default]`, or a `.txt` extension accidentally added by your text editor.\n:::\n\n\n# Available Datasets\n\nThe platform provides 18 base tables and 12 pre-built views. The table below is a quick reference; for full column-level details, query the `datasets_catalogue` and `columns_catalogue` programmatically (see [Querying the Data Catalogue](#querying-the-data-catalogue)).\n\n## Base Tables\n\n| Name | Description | Spatial | Approx. Rows |\n|------|-------------|:-------:|-------------:|\n| `bdline_ua_lep_diss_tbl` | Boundary line UA LEP dissolved | Yes | 1 |\n| `bdline_ua_lep_tbl` | Boundary line UA LEP | Yes | 5 |\n| `bdline_ua_weca_diss_tbl` | Boundary line UA WECA dissolved | Yes | 1 |\n| `bdline_ward_lep_tbl` | Boundary line wards LEP | Yes | 70 |\n| `boundary_lookup_tbl` | Boundary lookup (LA/ward/LSOA codes) | -- | 700 |\n| `ca_boundaries_bgc_tbl` | Combined authority boundaries (BGC) | Yes | 11 |\n| `ca_la_lookup_tbl` | Combined authority to local authority lookup | -- | 106 |\n| `codepoint_open_lep_tbl` | Codepoint Open postcodes LEP | Yes | 49,000 |\n| `eng_lsoa_imd_tbl` | English LSOA Index of Multiple Deprivation | -- | 33,000 |\n| `iod2025_tbl` | Index of Deprivation 2025 | -- | 33,000 |\n| `la_ghg_emissions_tbl` | Local authority greenhouse gas emissions | -- | 27,000 |\n| `la_ghg_emissions_wide_tbl` | LA GHG emissions (wide format) | -- | 6,000 |\n| `lsoa_2021_lep_tbl` | LSOA 2021 boundaries (LEP area) | Yes | 700 |\n| `open_uprn_lep_tbl` | Open UPRN addresses (LEP area) | Yes | 700,000 |\n| `postcode_centroids_tbl` | Postcode centroids (all England) | -- | 1,800,000 |\n| `raw_domestic_epc_certificates_tbl` | Domestic EPC certificates (raw) | -- | 19,000,000 |\n| `raw_non_domestic_epc_certificates_tbl` | Non-domestic EPC certificates (raw) | -- | 900,000 |\n| `uk_lsoa_tenure_tbl` | UK LSOA tenure (Census 2021) | -- | 35,000 |\n\n: {tbl-colwidths=\"[30,40,10,20]\"}\n\n## Views (WECA-filtered)\n\nThe catalogue includes 12 views. Four are source views that reshape or join data; the remaining eight pre-filter data to the four WECA local authorities (Bath and North East Somerset, Bristol, North Somerset, South Gloucestershire). Views have names ending in `_weca_vw`, `_vw`, or `_ods_vw`.\n\n| Name | Description |\n|------|-------------|\n| `ca_la_lookup_inc_ns_vw` | CA/LA lookup including North Somerset |\n| `weca_lep_la_vw` | WECA LEP local authorities (4 rows) |\n| `ca_la_ghg_emissions_sub_sector_ods_vw` | GHG emissions joined with CA/LA lookup |\n| `epc_domestic_vw` | Domestic EPC with derived fields (construction year, tenure) |\n| `la_ghg_emissions_weca_vw` | GHG emissions filtered to WECA LAs |\n| `la_ghg_emissions_wide_weca_vw` | GHG emissions (wide) filtered to WECA LAs |\n| `raw_domestic_epc_weca_vw` | Domestic EPC filtered to WECA LAs |\n| `raw_non_domestic_epc_weca_vw` | Non-domestic EPC filtered to WECA LAs |\n| `boundary_lookup_weca_vw` | Boundary lookup filtered to WECA LAs |\n| `postcode_centroids_weca_vw` | Postcode centroids filtered to WECA LAs |\n| `iod2025_weca_vw` | Index of Deprivation 2025 filtered to WECA LAs |\n| `ca_la_lookup_weca_vw` | CA/LA lookup filtered to WECA LAs |\n\n: {tbl-colwidths=\"[40,60]\"}\n\n::: {.callout-note}\nRow counts are approximate. For exact current counts, query `datasets_catalogue` in DuckLake (see [Section 5.5](#querying-the-data-catalogue)).\n:::\n\n# Accessing Data via Pins\n\nThe `pins` package is the primary way to read data from the platform into R. All datasets are stored as versioned parquet files on S3.\n\n## Creating a Board\n\nConnect to the S3 board. You only need to do this once per session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pins)\n\nboard <- board_s3(\n  bucket = \"stevecrawshaw-bucket\",\n  prefix = \"pins/\",\n  region = \"eu-west-2\",\n  versioned = TRUE\n)\n```\n:::\n\n\nThe `versioned = TRUE` parameter means you can access previous versions of datasets if needed.\n\n## Listing Datasets\n\nSee all available datasets on the board:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npin_list(board)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"bdline_ua_lep_diss_tbl\"               \n [2] \"bdline_ua_lep_tbl\"                    \n [3] \"bdline_ua_weca_diss_tbl\"              \n [4] \"bdline_ward_lep_tbl\"                  \n [5] \"boundary_lookup_tbl\"                  \n [6] \"ca_boundaries_bgc_tbl\"                \n [7] \"ca_la_lookup_tbl\"                     \n [8] \"codepoint_open_lep_tbl\"               \n [9] \"columns_catalogue\"                    \n[10] \"datasets_catalogue\"                   \n[11] \"eng_lsoa_imd_tbl\"                     \n[12] \"iod2025_tbl\"                          \n[13] \"la_ghg_emissions_tbl\"                 \n[14] \"la_ghg_emissions_wide_tbl\"            \n[15] \"lsoa_2021_lep_tbl\"                    \n[16] \"open_uprn_lep_tbl\"                    \n[17] \"postcode_centroids_tbl\"               \n[18] \"raw_domestic_epc_certificates_tbl\"    \n[19] \"raw_non_domestic_epc_certificates_tbl\"\n[20] \"uk_lsoa_tenure_tbl\"                   \n```\n\n\n:::\n:::\n\n\n## Reading a Dataset\n\nRead a dataset directly into an R data frame with `pin_read()`. Start with `ca_la_lookup_tbl` -- it is small (106 rows) and loads instantly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- pin_read(board, \"ca_la_lookup_tbl\")\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A data frame: 6 Ã— 5\n  LAD25CD   LAD25NM    CAUTH25CD CAUTH25NM          ObjectId\n* <chr>     <chr>      <chr>     <chr>                 <dbl>\n1 E08000001 Bolton     E47000001 Greater Manchester        1\n2 E08000002 Bury       E47000001 Greater Manchester        2\n3 E08000003 Manchester E47000001 Greater Manchester        3\n4 E08000004 Oldham     E47000001 Greater Manchester        4\n5 E08000005 Rochdale   E47000001 Greater Manchester        5\n6 E08000006 Salford    E47000001 Greater Manchester        6\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClasses 'tbl' and 'data.frame':\t106 obs. of  5 variables:\n $ LAD25CD  : chr  \"E08000001\" \"E08000002\" \"E08000003\" \"E08000004\" ...\n $ LAD25NM  : chr  \"Bolton\" \"Bury\" \"Manchester\" \"Oldham\" ...\n $ CAUTH25CD: chr  \"E47000001\" \"E47000001\" \"E47000001\" \"E47000001\" ...\n $ CAUTH25NM: chr  \"Greater Manchester\" \"Greater Manchester\" \"Greater Manchester\" \"Greater Manchester\" ...\n $ ObjectId : num  1 2 3 4 5 6 7 8 9 10 ...\n```\n\n\n:::\n:::\n\n\n## Accessing Metadata\n\nEvery dataset carries metadata including a description and column-level documentation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta <- pin_meta(board, \"ca_la_lookup_tbl\")\nmeta$title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Combined authority local authority boundaries\"\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta$user$columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$LAD25CD\n[1] \"Local authority district code 2025\"\n\n$LAD25NM\n[1] \"Local authority district name 2025\"\n\n$CAUTH25CD\n[1] \"Combined authority code 2025\"\n\n$CAUTH25NM\n[1] \"Combined authority name 2025\"\n\n$ObjectId\n[1] \"Objectid\"\n```\n\n\n:::\n:::\n\n\nColumn descriptions come from the data catalogue and are embedded in each pin's metadata. Use `meta$user$column_types` to see the data types.\n\n## Large Datasets\n\nThe `raw_domestic_epc_certificates_tbl` dataset contains approximately 19 million rows and is stored as a multi-file pin (multiple parquet files). For this table, `pin_read()` may be slow or fail. Use `arrow::open_dataset()` instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\n\n# Download the parquet files\npaths <- pin_download(board, \"raw_domestic_epc_certificates_tbl\")\n\n# Open as an Arrow dataset (lazy -- does not load into memory)\nds <- open_dataset(paths)\n\n# Query efficiently with dplyr\nlibrary(dplyr)\nbristol_epcs <- ds |>\n  filter(local_authority == \"Bristol, City of\") |>\n  select(address1, current_energy_rating, potential_energy_rating) |>\n  collect()\n```\n:::\n\n\n::: {.callout-tip}\nFor any dataset, you can use `pin_download()` followed by `arrow::read_parquet()` if you prefer working with Arrow tables directly. This is also the required approach for spatial datasets (see [Section 6](#working-with-spatial-data)).\n:::\n\n\n# Querying the DuckLake Catalogue\n\nDuckLake provides SQL access to the same datasets. This is useful when you need to join tables, run aggregate queries, use pre-built WECA views, or browse the data catalogue. All DuckLake SQL runs through the DuckDB CLI (not the R duckdb package).\n\n## Installing Extensions\n\nOpen the DuckDB CLI and install the required extensions. This only needs to be done once -- extensions are cached locally.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nINSTALL ducklake;\nLOAD ducklake;\n\nINSTALL httpfs;\nLOAD httpfs;\n\nINSTALL aws;\nLOAD aws;\n```\n:::\n\n\nFor spatial queries, also install:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nINSTALL spatial;\nLOAD spatial;\n```\n:::\n\n\n## Attaching the Catalogue\n\nEach time you open a DuckDB CLI session, you need to create an S3 credential and attach the catalogue.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n-- Create S3 credential (uses your ~/.aws/credentials file)\nCREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain);\n\n-- Attach the DuckLake catalogue\nATTACH 'ducklake:data/mca_env.ducklake' AS lake\n  (READ_ONLY, DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/');\n```\n:::\n\n\n::: {.callout-important}\n## You need the .ducklake file locally\nThe file `data/mca_env.ducklake` is a small metadata file that must be on your local machine. Clone the repository to get it:\n\n```bash\ngit clone <repo-url>\ncd ducklake\n```\n\nThe `.ducklake` file is only metadata (a few KB). The actual data lives on S3 and is fetched transparently when you run queries.\n:::\n\nTo see all available tables and views:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nUSE lake;\nSHOW TABLES;\n```\n:::\n\n\n## Basic SQL Queries\n\n**Select all rows from a small table:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT * FROM lake.ca_la_lookup_tbl LIMIT 5;\n```\n:::\n\n\n**Filter with WHERE:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT LAD25NM, population\nFROM lake.ca_la_lookup_tbl\nWHERE CAUTH25NM = 'West of England'\nORDER BY population DESC;\n```\n:::\n\n\n**Aggregate with GROUP BY:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT territorial_emissions_sector,\n       ROUND(SUM(territorial_emissions_kt_co2e), 1) AS total_kt\nFROM lake.la_ghg_emissions_tbl\nWHERE local_authority_name IN ('Bristol, City of', 'Bath and North East Somerset',\n                                'North Somerset', 'South Gloucestershire')\n  AND year = 2021\nGROUP BY territorial_emissions_sector\nORDER BY total_kt DESC;\n```\n:::\n\n\n**Join tables:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT l.LAD25NM, g.year,\n       ROUND(SUM(g.territorial_emissions_kt_co2e), 1) AS total_emissions\nFROM lake.la_ghg_emissions_tbl g\nJOIN lake.ca_la_lookup_tbl l ON g.local_authority_code = l.LAD25CD\nWHERE l.CAUTH25NM = 'West of England'\nGROUP BY l.LAD25NM, g.year\nORDER BY g.year DESC, total_emissions DESC\nLIMIT 8;\n```\n:::\n\n\n## Using Views\n\nWECA-filtered views pre-filter data to the four WECA local authorities, saving you from writing the same WHERE clause repeatedly.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n-- Instead of filtering manually:\nSELECT * FROM lake.la_ghg_emissions_tbl\nWHERE local_authority_code IN ('E06000022','E06000023','E06000024','E06000025');\n\n-- Use the WECA view directly:\nSELECT * FROM lake.la_ghg_emissions_weca_vw;\n```\n:::\n\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n-- List distinct authorities in a WECA view\nSELECT DISTINCT local_authority_code, local_authority_name\nFROM lake.la_ghg_emissions_weca_vw;\n```\n:::\n\n\n## Querying the Data Catalogue\n\nThe catalogue includes two metadata tables that describe every dataset and column programmatically.\n\n**List all datasets with descriptions and row counts:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT name, description, type, row_count\nFROM lake.datasets_catalogue\nORDER BY type, name;\n```\n:::\n\n\n**Explore columns for a specific table:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT column_name, data_type, description, example_1\nFROM lake.columns_catalogue\nWHERE table_name = 'la_ghg_emissions_tbl'\nORDER BY column_name;\n```\n:::\n\n\n**Find spatial datasets:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT name, geometry_type, crs, row_count\nFROM lake.datasets_catalogue\nWHERE geometry_type IS NOT NULL;\n```\n:::\n\n\n## Time Travel\n\nDuckLake keeps snapshots of your data. You can query a table as it existed at a previous version:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n-- Query the table at version 1\nSELECT COUNT(*) FROM lake.ca_la_lookup_tbl AT (VERSION => 1);\n\n-- List available snapshots\nSELECT * FROM lake.snapshots() ORDER BY snapshot_id DESC LIMIT 5;\n```\n:::\n\n\nSnapshots are retained for 90 days and then automatically cleaned up. Time travel is useful for auditing changes or recovering from accidental modifications.\n\n\n# Working with Spatial Data\n\nEight of the 18 datasets contain geometry columns and are stored as GeoParquet files on S3. These cover boundaries (UA, ward, LSOA), postcodes (Codepoint Open), addresses (UPRN), and combined authority areas.\n\n::: {.callout-warning}\n## Do not use sfarrow\nThe `sfarrow` package fails on GeoParquet files produced by DuckDB because DuckDB does not write CRS metadata into the GeoParquet `geo` key. Always use the `arrow` + `sf` approach described below.\n:::\n\n## Reading GeoParquet Pins\n\nSpatial pins must be read with `pin_download()` followed by `arrow::read_parquet()`, not with `pin_read()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download the GeoParquet file\npin_path <- pin_download(board, \"bdline_ua_lep_tbl\")\n\n# Read with arrow\narrow_tbl <- arrow::read_parquet(pin_path, as_data_frame = FALSE)\n```\n:::\n\n\n## Converting to sf Objects\n\nConvert the Arrow table to an `sf` object in two steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsf_obj <- sf::st_as_sf(as.data.frame(arrow_tbl))\n```\n:::\n\n\nThe two-step process (`as.data.frame()` then `st_as_sf()`) is needed because `sf` expects a data frame with a geometry column, and Arrow tables need explicit conversion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsf_obj\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 4 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 322292.8 ymin: 152770 xmax: 382605.5 ymax: 198319.9\nCRS:           NA\n                          name      code id                          shape\n1 Bath and North East Somerset E06000022  1 MULTIPOLYGON (((376262.8 17...\n2              City of Bristol E06000023  2 MULTIPOLYGON (((353416.3 18...\n3        South Gloucestershire E06000025  3 MULTIPOLYGON (((364881.8 19...\n4               North Somerset E06000024  4 MULTIPOLYGON (((349585 1781...\n```\n\n\n:::\n:::\n\n\n## Setting CRS\n\nNotice that the CRS shows as **NA** above. DuckDB does not embed CRS metadata into GeoParquet files, so you **must** set it explicitly after reading.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsf_obj <- sf::st_set_crs(sf_obj, 27700)\n```\n:::\n\n\nMost spatial tables use **EPSG:27700** (British National Grid). The one exception is `ca_boundaries_bgc_tbl`, which uses **EPSG:4326** (WGS84 latitude/longitude).\n\n| Dataset | CRS |\n|---------|-----|\n| `bdline_ua_lep_diss_tbl` | EPSG:27700 |\n| `bdline_ua_lep_tbl` | EPSG:27700 |\n| `bdline_ua_weca_diss_tbl` | EPSG:27700 |\n| `bdline_ward_lep_tbl` | EPSG:27700 |\n| `ca_boundaries_bgc_tbl` | **EPSG:4326** |\n| `codepoint_open_lep_tbl` | EPSG:27700 |\n| `lsoa_2021_lep_tbl` | EPSG:27700 |\n| `open_uprn_lep_tbl` | EPSG:27700 |\n\n: {tbl-colwidths=\"[50,50]\"}\n\nYou can also check the correct CRS from the pin metadata:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta <- pin_meta(board, \"bdline_ua_lep_tbl\")\nmeta$user$crs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"EPSG:27700\"\n```\n\n\n:::\n:::\n\n\n## Plotting a Quick Map\n\nOnce the CRS is set, you can plot directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sf_obj[\"name\"])\n```\n\n::: {.cell-output-display}\n![](analyst-guide_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nFor a more polished map using ggplot2:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sf_obj) +\n  geom_sf(aes(fill = name), colour = \"white\") +\n  theme_minimal() +\n  labs(title = \"WECA LEP Local Authorities\",\n       fill = \"Authority\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](analyst-guide_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n# Troubleshooting\n\n## \"Access Denied\" or region errors\n\n**Symptom:** S3 operations fail with \"Access Denied\", 403, or 301 redirect errors.\n\n**Cause:** The region is not set to `eu-west-2`, or credentials are missing/expired.\n\n**Fix:**\n\n1. Check `~/.aws/config` contains `region = eu-west-2`.\n2. In DuckDB, always specify `REGION 'eu-west-2'` in the secret definition.\n3. Verify your credentials have not expired -- contact the data owner for new keys if needed.\n\n## \"File not found\" when attaching DuckLake\n\n**Symptom:** `ATTACH 'ducklake:data/mca_env.ducklake'` fails with a file-not-found error.\n\n**Cause:** The `.ducklake` catalogue file must be on your local machine. It is not hosted on S3.\n\n**Fix:** Clone the repository and run DuckDB from the project root directory:\n\n```bash\ngit clone <repo-url>\ncd ducklake\nduckdb\n```\n\n## CRS shows as NA or OGC:CRS84\n\n**Symptom:** After reading a spatial dataset, `sf::st_crs()` returns NA, or geopandas shows `OGC:CRS84`.\n\n**Cause:** DuckDB does not embed CRS metadata into GeoParquet files.\n\n**Fix:** Set the CRS explicitly after reading. Most tables use EPSG:27700; `ca_boundaries_bgc_tbl` uses EPSG:4326. See the [CRS table](#setting-crs) above.\n\n## sfarrow fails on GeoParquet\n\n**Symptom:** `sfarrow::st_read_parquet()` throws an error about missing CRS or `geo` key.\n\n**Cause:** sfarrow requires full GeoParquet metadata including CRS, which DuckDB does not write.\n\n**Fix:** Use `arrow::read_parquet()` followed by `sf::st_as_sf()` instead. Do not use sfarrow with these files.\n\n## Python board_s3 path format\n\n**Symptom:** Python `board_s3()` fails with connection errors or \"bucket not found\".\n\n**Cause:** Python `pins` uses a different path format from R.\n\n**Fix:** Use `board_s3(\"stevecrawshaw-bucket/pins\", versioned=True)` -- the bucket and prefix are combined in a single string. Do not pass them as separate parameters.\n\n## Python pin_read fails on EPC table\n\n**Symptom:** `board.pin_read(\"raw_domestic_epc_certificates_tbl\")` throws an error in Python.\n\n**Cause:** The EPC table is a multi-file pin (exported in chunks). The Python `pins` library cannot handle multi-file pins with `pin_read`.\n\n**Fix:** Use `pyarrow` directly:\n\n```python\nimport pyarrow.dataset as ds\npaths = board.pin_download(\"raw_domestic_epc_certificates_tbl\")\ndataset = ds.dataset(paths, format=\"parquet\")\ndf = dataset.to_table().to_pandas()\n```\n\n## DuckLake extension not available in R\n\n**Symptom:** `dbExecute(con, \"INSTALL ducklake\")` fails from the R `duckdb` package.\n\n**Cause:** The R `duckdb` package (v1.4.4) does not include the `ducklake` extension.\n\n**Fix:** Use the DuckDB CLI for all DuckLake queries. For data frame access, use `pins` instead -- you do not need DuckLake for reading data into R.\n\n\n# Support and Contact\n\n::: {.callout-note}\n## Update before distributing\nReplace the placeholders below with your team's actual contact details before sharing this guide.\n:::\n\n- **Contact:** Steve Crawshaw\n- **Repository:** Report issues or request new datasets via the project [GitHub repository](https://github.com/stevecrawshaw/ducklake).\n- **AWS access requests:** Contact the Steve to receive credentials.\n\nFor urgent data access issues, check the [Troubleshooting](#troubleshooting) section first.\n\n\n# Appendix A: Python Equivalents {.appendix}\n\nThis appendix provides Python equivalents of the key R examples above. Python is a secondary access path -- R is the primary supported language.\n\n## Pins Access\n\n::: {.callout-important}\n## Python path format differs from R\nIn Python, `board_s3()` takes a single string `\"bucket/prefix\"` rather than separate `bucket` and `prefix` parameters. Note the absence of a trailing slash.\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nos.environ.setdefault(\"AWS_DEFAULT_REGION\", \"eu-west-2\")\n\nfrom pins import board_s3\n\n# Create board -- note the \"bucket/prefix\" format (no trailing slash)\nboard = board_s3(\"stevecrawshaw-bucket/pins\", versioned=True)\n\n# List datasets\nboard.pin_list()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Read a dataset\ndf = board.pin_read(\"ca_la_lookup_tbl\")\nprint(df.head())\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Read metadata\nmeta = board.pin_meta(\"ca_la_lookup_tbl\")\nprint(meta.title)\nprint(meta.user[\"columns\"])\n```\n:::\n\n\n::: {.callout-warning}\n`pin_read()` fails on multi-file pins (e.g., the EPC table). Use `pyarrow.dataset` as shown in the [Troubleshooting](#python-pin_read-fails-on-epc-table) section.\n:::\n\n## DuckLake Queries\n\nYou can run DuckLake SQL from Python using the `duckdb` package:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport duckdb\n\nconn = duckdb.connect()\n\n# Install and load extensions\nconn.execute(\"INSTALL ducklake; LOAD ducklake;\")\nconn.execute(\"INSTALL httpfs; LOAD httpfs;\")\nconn.execute(\"INSTALL aws; LOAD aws;\")\n\n# Create S3 credential\nconn.execute(\"CREATE SECRET (TYPE s3, REGION 'eu-west-2', PROVIDER credential_chain);\")\n\n# Attach the DuckLake catalogue\nconn.execute(\"\"\"\n    ATTACH 'ducklake:data/mca_env.ducklake' AS lake\n    (READ_ONLY, DATA_PATH 's3://stevecrawshaw-bucket/ducklake/data/');\n\"\"\")\n\n# Query\ndf = conn.execute(\"SELECT * FROM lake.ca_la_lookup_tbl LIMIT 5\").fetchdf()\nprint(df)\n```\n:::\n\n\n::: {.callout-note}\nUnlike R, the Python `duckdb` package does support the `ducklake` extension. You can run DuckLake queries directly from Python without the CLI.\n:::\n\n## Spatial Data with geopandas\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nos.environ.setdefault(\"AWS_DEFAULT_REGION\", \"eu-west-2\")\n\nfrom pins import board_s3\nimport geopandas as gpd\n\nboard = board_s3(\"stevecrawshaw-bucket/pins\", versioned=True)\n\n# Download and read GeoParquet\npath = board.pin_download(\"bdline_ua_lep_tbl\")\ngdf = gpd.read_parquet(path[0])\n\n# CRS may show as OGC:CRS84 -- override to the correct CRS\ngdf = gdf.set_crs(\"EPSG:27700\", allow_override=True)\n\n# Quick plot\ngdf.plot(column=\"name\", legend=True)\n```\n:::\n\n\n::: {.callout-warning}\ngeopandas may report the CRS as `OGC:CRS84` when GeoParquet lacks CRS metadata. Always use `set_crs()` with `allow_override=True` to set the correct CRS.\n:::\n\n\n# Appendix B: SQL Quick Reference {.appendix}\n\nA brief SQL primer for analysts who are familiar with R but new to SQL. All examples use DuckLake tables.\n\n**SELECT -- choose columns:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT LAD25NM, population\nFROM lake.ca_la_lookup_tbl;\n```\n:::\n\n\nR equivalent: `df |> select(LAD25NM, population)`\n\n**WHERE -- filter rows:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT LAD25NM, population\nFROM lake.ca_la_lookup_tbl\nWHERE CAUTH25NM = 'West of England';\n```\n:::\n\n\nR equivalent: `df |> filter(CAUTH25NM == \"West of England\")`\n\n**ORDER BY -- sort results:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT LAD25NM, population\nFROM lake.ca_la_lookup_tbl\nORDER BY population DESC;\n```\n:::\n\n\nR equivalent: `df |> arrange(desc(population))`\n\n**LIMIT -- return a fixed number of rows:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT * FROM lake.la_ghg_emissions_tbl LIMIT 10;\n```\n:::\n\n\nR equivalent: `df |> head(10)` or `df |> slice_head(n = 10)`\n\n**GROUP BY -- aggregate:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT CAUTH25NM, COUNT(*) AS n_las, SUM(population) AS total_pop\nFROM lake.ca_la_lookup_tbl\nGROUP BY CAUTH25NM\nORDER BY total_pop DESC;\n```\n:::\n\n\nR equivalent: `df |> group_by(CAUTH25NM) |> summarise(n_las = n(), total_pop = sum(population))`\n\n**JOIN -- combine tables:**\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT l.LAD25NM, g.year, g.territorial_emissions_kt_co2e\nFROM lake.la_ghg_emissions_tbl g\nJOIN lake.ca_la_lookup_tbl l\n  ON g.local_authority_code = l.LAD25CD\nWHERE l.CAUTH25NM = 'West of England'\n  AND g.year = 2021;\n```\n:::\n\n\nR equivalent: `left_join(ghg, lookup, by = c(\"local_authority_code\" = \"LAD25CD\"))`\n\n**Common aggregate functions:**\n\n| SQL | R equivalent | Description |\n|-----|-------------|-------------|\n| `COUNT(*)` | `n()` | Number of rows |\n| `SUM(col)` | `sum(col)` | Total |\n| `AVG(col)` | `mean(col)` | Average |\n| `MIN(col)` | `min(col)` | Minimum |\n| `MAX(col)` | `max(col)` | Maximum |\n| `ROUND(col, 2)` | `round(col, 2)` | Round to N decimals |\n\n: {tbl-colwidths=\"[20,25,55]\"}\n",
    "supporting": [
      "analyst-guide_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}