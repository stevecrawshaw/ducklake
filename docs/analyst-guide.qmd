---
title: "WECA Data Platform: Analyst Guide"
subtitle: "Accessing shared datasets via pins and DuckLake"
author: "West of England Combined Authority"
date: last-modified
title-block-banner: true
format:
  html:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 3
    code-fold: false
    embed-resources: true
  weca-report-typst:
    toc: true
    toc-depth: 3
execute:
  echo: true
  eval: false
  warning: false
---

![](weca_logo.jpg){width=200px fig-align="left"}

# Introduction

WECA's shared data platform provides **18 curated datasets** covering local authority lookups, greenhouse gas emissions, energy performance certificates, broadband coverage, IMD deprivation, business counts, and spatial boundaries. All data is stored on Amazon S3 and accessible through two complementary routes:

- **Pins (R or Python):** Read datasets directly into data frames. Best for quick exploratory analysis, filtering, and visualisation.
- **DuckLake (SQL via DuckDB CLI):** Query datasets with SQL, join across tables, use pre-built WECA-filtered views, and browse the data catalogue. Best for complex queries and when you need to work across multiple tables.

Both routes read from the same underlying data. Choose whichever fits your workflow -- or use both.

::: {.callout-tip}
## The 10-minute promise
If you follow the Prerequisites section below, you will be able to read your first dataset within 10 minutes.
:::

This guide covers everything you need: package installation, AWS credential setup, reading data via pins, querying the DuckLake catalogue with SQL, working with spatial data, and troubleshooting common issues. Python equivalents are provided in [Appendix A](#appendix-a-python-equivalents).


# Prerequisites and Setup

Before accessing data, you need three things: R packages, the DuckDB command-line tool (for SQL access), and AWS credentials. This section walks through each.

## R Packages

Install the core packages. This guide assumes you already have the tidyverse installed.

```{r}
install.packages(c("pins", "arrow", "sf", "duckdb", "DBI"))
```

| Package | Purpose |
|---------|---------|
| `pins` | Connect to the S3 board, list/read/download datasets |
| `arrow` | Read parquet files (required for large and spatial datasets) |
| `sf` | Handle spatial data (GeoParquet to sf objects) |
| `duckdb` | DuckDB database driver for R (used for local queries) |
| `DBI` | Database interface (works with duckdb) |

: {tbl-colwidths="[20,80]"}

## DuckDB Installation

The DuckDB command-line interface (CLI) is required for querying the DuckLake catalogue. The R `duckdb` package (v1.4.4) does not include the `ducklake` extension, so SQL queries against the catalogue must be run through the CLI.

1. Download the DuckDB CLI from <https://duckdb.org/docs/installation/>.
2. Place the `duckdb` executable somewhere on your PATH.
3. Verify installation:

```{bash}
duckdb --version
```

```text
v1.4.1 (Groundhog Day)
```

::: {.callout-note}
The DuckDB CLI is only needed for DuckLake SQL queries ([Section 5](#querying-the-ducklake-catalogue)). For reading data via pins, you only need the R packages above.
:::

## AWS Credentials

You need AWS credentials to access the shared S3 bucket (`stevecrawshaw-bucket`). Request an access key ID and secret access key from the data owner (distributed via Keeper).

### Step 1: Create the credentials file

Create the file at:

- **Windows:** `C:\Users\USERNAME\.aws\credentials`
- **Linux/Mac:** `~/.aws/credentials`

If the `.aws` directory does not exist, create it first. Paste the following content, replacing the placeholders with your actual keys:

```ini
[default]
aws_access_key_id = <YOUR_ACCESS_KEY_ID>
aws_secret_access_key = <YOUR_SECRET_ACCESS_KEY>
```

### Step 2: Create the config file

Create the file at:

- **Windows:** `C:\Users\USERNAME\.aws\config`
- **Linux/Mac:** `~/.aws/config`

Paste the following content:

```ini
[default]
region = eu-west-2
output = json
```

::: {.callout-warning}
## Region must be eu-west-2
The S3 bucket is in the `eu-west-2` (London) region. Using any other region will cause "Access Denied" errors or 301 redirects. Always set `region = eu-west-2`.
:::

### Step 3: Verify access

Pick whichever tool you use day-to-day. A successful result from any one method confirms your credentials are working.

**R (pins):**

```{r}
library(pins)
board <- board_s3(bucket = "stevecrawshaw-bucket", region = "eu-west-2")
pin_list(board)
```

```text
 [1] "bdline_ua_lep_diss_tbl"             "bdline_ua_lep_tbl"
 [3] "bdline_ua_weca_diss_tbl"            "bdline_ward_lep_tbl"
 [5] "ca_boundaries_bgc_tbl"              "ca_la_lookup_tbl"
 ...
```

**DuckDB CLI:**

```{sql}
INSTALL aws; LOAD aws;
CREATE OR REPLACE SECRET (
    TYPE s3, PROVIDER credential_chain,
    CHAIN config, REGION 'eu-west-2'
);
SELECT * FROM glob('s3://stevecrawshaw-bucket/*');
```

**AWS CLI (optional):**

```{bash}
aws s3 ls s3://stevecrawshaw-bucket/
```

::: {.callout-tip}
## Troubleshooting credentials
If you get "Access Denied" or "No credentials" errors, see the [Troubleshooting](#troubleshooting) section. Common causes: wrong region, missing profile name `[default]`, or a `.txt` extension accidentally added by your text editor.
:::


# Available Datasets

The platform provides 18 base tables and 12 pre-built views. The table below is a quick reference; for full column-level details, query the `datasets_catalogue` and `columns_catalogue` programmatically (see [Querying the Data Catalogue](#querying-the-data-catalogue)).

## Base Tables

| Name | Description | Spatial | Approx. Rows |
|------|-------------|:-------:|-------------:|
| `bdline_ua_lep_diss_tbl` | Boundary line UA LEP dissolved | Yes | 1 |
| `bdline_ua_lep_tbl` | Boundary line UA LEP | Yes | 5 |
| `bdline_ua_weca_diss_tbl` | Boundary line UA WECA dissolved | Yes | 1 |
| `bdline_ward_lep_tbl` | Boundary line wards LEP | Yes | 70 |
| `ca_boundaries_bgc_tbl` | Combined authority boundaries (BGC) | Yes | 11 |
| `ca_la_lookup_tbl` | Combined authority to local authority lookup | -- | 106 |
| `codepoint_open_lep_tbl` | Codepoint Open postcodes LEP | Yes | 49,000 |
| `imd_2019_lsoa_lep_tbl` | IMD 2019 by LSOA (LEP area) | -- | 700 |
| `la_broadband_coverage_tbl` | Local authority broadband coverage | -- | 440 |
| `la_business_counts_tbl` | Local authority business counts | -- | 3,600 |
| `la_ghg_emissions_tbl` | Local authority greenhouse gas emissions | -- | 27,000 |
| `la_house_price_earnings_ratio_tbl` | House price to earnings ratio by LA | -- | 2,100 |
| `la_imd_2019_summary_tbl` | IMD 2019 summary by local authority | -- | 800 |
| `lsoa_2021_lep_tbl` | LSOA 2021 boundaries (LEP area) | Yes | 700 |
| `oa_2021_lsoa_lep_lookup_tbl` | OA 2021 to LSOA LEP lookup | -- | 3,300 |
| `open_uprn_lep_tbl` | Open UPRN addresses (LEP area) | Yes | 700,000 |
| `raw_domestic_epc_certificates_tbl` | Domestic EPC certificates (raw) | -- | 19,000,000 |
| `uk_hpi_tbl` | UK House Price Index | -- | 100,000 |

: {tbl-colwidths="[30,40,10,20]"}

## Views (WECA-filtered)

The catalogue includes 12 views, most of which pre-filter data to the four WECA local authorities (Bath and North East Somerset, Bristol, North Somerset, South Gloucestershire). Views have names ending in `_weca_vw` or `_source_vw`.

| Name | Description |
|------|-------------|
| `la_ghg_emissions_weca_vw` | GHG emissions filtered to WECA LAs |
| `la_broadband_coverage_weca_vw` | Broadband coverage filtered to WECA LAs |
| `la_business_counts_weca_vw` | Business counts filtered to WECA LAs |
| `weca_lep_la_vw` | WECA LEP local authority lookup (4 rows) |

: {tbl-colwidths="[40,60]"}

::: {.callout-note}
Row counts are approximate. For exact current counts, query `datasets_catalogue` in DuckLake (see [Section 5.5](#querying-the-data-catalogue)).
:::

# Accessing Data via Pins

<!-- Content: Primary data access method for R analysts.
     All examples use the pins package with S3 board. -->

## Creating a Board

<!-- Content: board_s3() with bucket, prefix, region, versioned params.
     Bucket: stevecrawshaw-bucket, prefix: pins/, region: eu-west-2. -->

## Listing Datasets

<!-- Content: pin_list(board) to see all available datasets.
     Show expected output (list of 18+ pin names). -->

## Reading a Dataset

<!-- Content: pin_read(board, "table_name") for standard tables.
     Use ca_la_lookup_tbl as first example (small, quick).
     Show glimpse/str of result. -->

## Accessing Metadata

<!-- Content: pin_meta(board, "table_name") for descriptions and column info.
     Show meta$title, meta$description, meta$user$columns.
     Explain that metadata comes from the data catalogue. -->

## Large Datasets

<!-- Content: The EPC table is too large for pin_read (multi-file pin).
     Use pin_download() + arrow::read_parquet() or arrow::open_dataset() instead.
     Show the fallback pattern with raw_domestic_epc_certificates_tbl. -->

# Querying the DuckLake Catalogue

<!-- Content: SQL-based access via DuckDB CLI.
     For analysts comfortable with SQL, or when complex queries/joins are needed. -->

## Installing Extensions

<!-- Content: INSTALL ducklake; LOAD ducklake;
     INSTALL httpfs; LOAD httpfs; (for S3 access)
     Note these are one-time installations. -->

## Attaching the Catalogue

<!-- Content: CREATE SECRET for AWS credentials.
     ATTACH 'ducklake:data/mca_env.ducklake' AS lake syntax.
     Explain: analysts must have the .ducklake file locally (clone the repo).
     DATA_PATH points to S3 where actual data lives. -->

## Basic SQL Queries

<!-- Content: SELECT, WHERE, JOIN, GROUP BY examples using DuckLake tables.
     Use familiar tables (ca_la_lookup_tbl, la_ghg_emissions_tbl).
     Show practical queries analysts would actually run. -->

## Using Views

<!-- Content: WECA-filtered views (e.g., la_ghg_emissions_weca_vw).
     Explain views pre-filter to WECA local authorities.
     SHOW TABLES to list available views. -->

## Querying the Data Catalogue

<!-- Content: SELECT from datasets_catalogue and columns_catalogue.
     Show how to discover dataset descriptions, column meanings, row counts.
     This is the programmatic alternative to the quick reference table above. -->

## Time Travel

<!-- Content: Brief syntax example — not a full section.
     SELECT ... FROM table AT (VERSION => N) syntax.
     Mention 90-day snapshot retention policy. -->

# Working with Spatial Data

<!-- Content: 8 spatial datasets available as GeoParquet pins.
     Important: do NOT use sfarrow (fails on DuckDB GeoParquet). -->

## Reading GeoParquet Pins

<!-- Content: pin_download() to get GeoParquet file path.
     arrow::read_parquet() to read (not pin_read, not sfarrow). -->

## Converting to sf Objects

<!-- Content: sf::st_as_sf(as.data.frame(arrow_tbl)) pattern.
     Explain why this two-step process is needed. -->

## Setting CRS

<!-- Content: CRITICAL — DuckDB GeoParquet does not embed CRS metadata.
     sf will show CRS as NA. Analysts MUST set CRS explicitly.
     Most spatial tables use EPSG:27700 (British National Grid).
     ca_boundaries_bgc_tbl uses EPSG:4326 (WGS84).
     Show sf::st_set_crs() and how to check pin metadata for correct CRS. -->

## Plotting a Quick Map

<!-- Content: Simple plot(sf_obj["column"]) example.
     Optional: ggplot2 + geom_sf for a nicer map.
     Use bdline_ua_lep_tbl or similar recognisable boundary. -->

# Troubleshooting

<!-- Content: Dedicated troubleshooting section covering common issues.
     - AWS credential errors (expired, wrong region, not configured)
     - "file not found" when attaching DuckLake (need .ducklake file locally)
     - sfarrow errors on GeoParquet (use arrow + sf instead)
     - CRS shows as NA or OGC:CRS84 (must set explicitly)
     - Python pin_read fails on EPC table (multi-file pin — use arrow fallback)
     - DuckLake extension not available in R (use DuckDB CLI)
     - s3fs version warning in Python (cosmetic, can ignore) -->

# Support and Contact

<!-- Content: Who to contact for data platform questions.
     Slack channel: [#data-platform] (placeholder — fill before publishing).
     Named contact: [Data Platform Team] (placeholder).
     GitHub repo link for reporting issues. -->

# Appendix A: Python Equivalents {.appendix}

<!-- Content: Python versions of the key R examples above.
     For analysts who prefer Python or need to integrate with Python workflows. -->

## Pins Access

<!-- Content: board_s3("stevecrawshaw-bucket/pins", versioned=True) — note the
     different path format from R (bucket/prefix, no separate params).
     board.pin_list(), board.pin_read(), board.pin_meta().
     Note: pin_read fails on multi-file pins — use arrow fallback. -->

## DuckLake Queries

<!-- Content: Python DuckDB connection for SQL queries.
     import duckdb; conn = duckdb.connect()
     Same SQL as the R/CLI examples above.
     INSTALL/LOAD ducklake, ATTACH, SELECT. -->

## Spatial Data with geopandas

<!-- Content: geopandas.read_parquet() for GeoParquet pins.
     CRS may show as OGC:CRS84 — override with set_crs("EPSG:27700", allow_override=True).
     Quick plot with gdf.plot(). -->

# Appendix B: SQL Quick Reference {.appendix}

<!-- Content: Brief SQL primer for analysts who know R but not SQL.
     Cover: SELECT, FROM, WHERE, JOIN, GROUP BY, ORDER BY, LIMIT.
     Use DuckLake table examples throughout.
     Not a full SQL course — just enough to query the catalogue effectively. -->
